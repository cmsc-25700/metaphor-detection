{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "replicate_mohx_sequence.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_bVSLfpL12W",
        "outputId": "f23aa410-b82c-48d6-d1b4-afaf04c212ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "ROOT = '/content/drive'\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from os.path import join \n",
        "repo_dir = '/content/drive/MyDrive/metaphor-detection'"
      ],
      "metadata": {
        "id": "UVpWOogpMTE9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## directories for resources\n",
        "data_dir = repo_dir + '/resources/metaphor-in-context/data/'\n",
        "glove_dir = repo_dir + '/resources/glove/'\n",
        "elmo_dir = repo_dir + '/resources/elmo/'"
      ],
      "metadata": {
        "id": "3Cco5NznMXLb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## installing the requirements\n",
        "%cd 'drive/MyDrive/metaphor-detection/' \n",
        "#!pip install allennlp\n",
        "#!pip install -r gao-g-requirements.txt\n",
        "#!pip install --upgrade google-cloud-storage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXgKE0zYMYUa",
        "outputId": "69c5eca6-f985-4761-a252-85c28f5787a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/metaphor-detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from core.gao_files.sequence.model import RNNSequenceModel\n",
        "import time\n",
        "import matplotlib\n",
        "from core.gao_files.sequence.util import *\n",
        "from core.data.gao_data import *\n",
        "import h5py\n",
        "import math\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "76Jhoq62MbdE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preperation"
      ],
      "metadata": {
        "id": "Jht9gQOrNPpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Read MOH-x Data \n",
        "data_dir = os.path.join(\"resources\", \"metaphor-in-context\", \"data\")\n",
        "data_container = ExperimentData(data_dir)\n",
        "data_container.read_moh_x_data(to_pandas = False)\n",
        "moh_x_data = data_container.moh_x_formatted_svo_cleaned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLachZPZNIY-",
        "outputId": "52e99ec1-72eb-48bd-d3e8-8354e7792a00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MOH-X formatted svo nrow: 647\n",
            "MOH-X formatted svo cleaned nrow: 647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "moh_x_data_formatted = []"
      ],
      "metadata": {
        "id": "FFl8PNbFNPAY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_mohx = []\n",
        "\n",
        "with open(data_dir + '/MOH-X/MOH-X_formatted_svo_cleaned.csv') as f:\n",
        "    # arg1  \targ2\tverb\tsentence\tverb_idx\tlabel\n",
        "    lines = csv.reader(f)\n",
        "    next(lines)\n",
        "    for line in lines:\n",
        "        sentence = line[3]\n",
        "        label_seq = [0] * len(sentence.split())\n",
        "        pos_seq = [0] * len(label_seq)\n",
        "        verb_idx = int(line[4])\n",
        "        verb_label = int(line[5])\n",
        "        label_seq[verb_idx] = verb_label\n",
        "        pos_seq[verb_idx] = 1   # idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n",
        "        raw_mohx.append([sentence.strip(), label_seq, pos_seq])\n",
        "\n"
      ],
      "metadata": {
        "id": "eFQqXuJrNMWy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Sample data format\n",
        "raw_mohx[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsXnc8bhN8BW",
        "outputId": "3ca3161e-9aae-4256-833c-617a548ba4ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He absorbed the knowledge or beliefs of his tribe .',\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = get_vocab(raw_mohx)\n",
        "word2idx, idx2word = get_word2idx_idx2word(vocab)\n",
        "glove_embeddings = get_embedding_matrix(glove_dir + 'glove840B300d.txt', \n",
        "                                        word2idx, \n",
        "                                        idx2word, \n",
        "                                        normalization=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkqQ0wwKN91Q",
        "outputId": "bc35bfc4-7187-47ef-d7cc-6b710e4900e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size:  1732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2196017/2196017 [00:47<00:00, 45921.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pre-trained word vectors loaded:  1730\n",
            "Embeddings mean:  -0.0001946780103025958\n",
            "Embeddings stdev:  0.3732253909111023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "elmo_embeddings = h5py.File(elmo_dir + 'MOH-X_cleaned.hdf5', 'r')"
      ],
      "metadata": {
        "id": "DktxEnQcPPD0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Embedding the data with shuffled randomly\n",
        "random.seed(0)\n",
        "random.shuffle(raw_mohx)\n",
        "\n",
        "# second argument is the post sequence, which we don't need\n",
        "embedded_mohx = [[embed_indexed_sequence(data[0], data[2], word2idx,\n",
        "                                      glove_embeddings, elmo_embeddings, None),\n",
        "                       data[2], data[1]]\n",
        "                      for data in raw_mohx]"
      ],
      "metadata": {
        "id": "X7VGlkE9OSO6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [data[0] for data in embedded_mohx]\n",
        "poss = [data[1] for data in embedded_mohx]\n",
        "labels = [data[2] for data in embedded_mohx]"
      ],
      "metadata": {
        "id": "-5dwd9ovPZ8u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Fold Training"
      ],
      "metadata": {
        "id": "g5wGKGTkPu4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Data Length is {len(raw_mohx)}\")\n",
        "NUMBER_FOLD = 10\n",
        "fold_size = round(len(raw_mohx) / NUMBER_FOLD)\n",
        "print(f\"Each fold size is {fold_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJN8TH_4PwFG",
        "outputId": "1ded377d-d346-456a-d5ec-5655672fcfcd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Length is 647\n",
            "Each fold size is 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folds = []\n",
        "for i in range(NUMBER_FOLD):\n",
        "    folds.append((sentences[i * fold_size:(i + 1) * fold_size],\n",
        "                  poss[i * fold_size:(i + 1) * fold_size],\n",
        "                  labels[i * fold_size: (i + 1) * fold_size]))\n",
        "idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n"
      ],
      "metadata": {
        "id": "OkZChnaKP0E-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_f1s = []\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "BATCH_SIZE = 10\n",
        "using_GPU = True\n",
        "NUM_EPOCHS = 10\n",
        "for i in range(NUMBER_FOLD):\n",
        "    ### DATA BATCHING\n",
        "    training_sentences = []\n",
        "    training_labels = []\n",
        "    training_poss = []\n",
        "    for j in range(NUMBER_FOLD):\n",
        "        if j != i:\n",
        "            training_sentences.extend(folds[j][0])\n",
        "            training_poss.extend(folds[j][1])\n",
        "            training_labels.extend(folds[j][1])\n",
        "    training_dataset_mohX = TextDatasetWithGloveElmoSuffix(training_sentences, \n",
        "                                                            training_poss, \n",
        "                                                            training_labels)\n",
        "    val_dataset_mohX = TextDatasetWithGloveElmoSuffix(folds[i][0], \n",
        "                                                       folds[i][1], \n",
        "                                                       folds[i][2])\n",
        "\n",
        "    # Data-related hyperparameters\n",
        "    # Set up a DataLoader for the training, validation, and test dataset\n",
        "    train_dataloader_mohX = DataLoader(dataset=training_dataset_mohX, \n",
        "                                       batch_size=BATCH_SIZE, \n",
        "                                       shuffle=True,\n",
        "                                      collate_fn=TextDatasetWithGloveElmoSuffix\n",
        "                                                .collate_fn)\n",
        "    val_dataloader_mohX = DataLoader(dataset=val_dataset_mohX, \n",
        "                                     batch_size=BATCH_SIZE, \n",
        "                                     shuffle=False,\n",
        "                                      collate_fn=TextDatasetWithGloveElmoSuffix\n",
        "                                                .collate_fn)\n",
        "    rnn_seq = RNNSequenceModel(num_classes=2, \n",
        "                                    embedding_dim=300+1024, \n",
        "                                    hidden_size=300, num_layers=1, \n",
        "                                    bidir=True,\n",
        "                                    dropout1=0.5, dropout2=0, dropout3=0)\n",
        "    nll_criterion = nn.NLLLoss()\n",
        "    if using_GPU:\n",
        "        rnn_seq = rnn_seq.cuda()\n",
        "        nll_criterion = nll_criterion.cuda()\n",
        "\n",
        "    rnn_seq_optimizer = optim.Adam(rnn_seq.parameters(), lr=0.001)\n",
        "    #### TRAIN ####\n",
        "    performance_matrix = None\n",
        "    training_loss = []\n",
        "    val_loss = []\n",
        "    val_p = []\n",
        "    val_r = []\n",
        "    val_acc = []\n",
        "    training_f1 = []\n",
        "    val_f1 = []\n",
        "    train_dataloader = train_dataloader_mohX\n",
        "    val_dataloader = val_dataloader_mohX\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        num_iter = 0\n",
        "        model_index = 0\n",
        "        comparable = []\n",
        "        print(\"-----Starting epoch {}------\".format(epoch + 1))\n",
        "        now = time.time()\n",
        "        for (__, example_text, example_lengths, labels) in train_dataloader:\n",
        "            example_text = Variable(example_text)\n",
        "            example_lengths = Variable(example_lengths)\n",
        "            labels = Variable(labels)\n",
        "            if using_GPU:\n",
        "                example_text = example_text.cuda()\n",
        "                example_lengths = example_lengths.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            # predicted shape: (batch_size, 2)\n",
        "            predicted = rnn_seq(example_text, example_lengths)\n",
        "            batch_loss = nll_criterion(predicted.view(-1, 2), labels.view(-1))\n",
        "            rnn_seq_optimizer.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            rnn_seq_optimizer.step()\n",
        "            num_iter += 1\n",
        "            # Calculate validation and training set loss and accuracy every 200 gradient updates\n",
        "            if num_iter % 50 == 0:\n",
        "                avg_eval_loss, performance_matrix = evaluate(idx2pos, \n",
        "                                                             val_dataloader_mohX, \n",
        "                                                             rnn_seq,\n",
        "                                                             nll_criterion, \n",
        "                                                             using_GPU)\n",
        "                val_loss.append(avg_eval_loss)\n",
        "                val_p.append(performance_matrix[1][0])\n",
        "                val_r.append(performance_matrix[1][1])\n",
        "                val_f1.append(performance_matrix[1][2])\n",
        "                val_acc.append(performance_matrix[1][3])\n",
        "\n",
        "                #print(\"####Iteration {}. Validation Loss {}.#####\".format(num_iter, avg_eval_loss))\n",
        "                # filename = '../models/LSTMSuffixElmoAtt_???_all_iter_' + str(num_iter) + '.pt'\n",
        "                # torch.save(rnn_clf, filename)\n",
        "                 \n",
        "    print('val_f1: ', val_f1)\n",
        "    idx = 0\n",
        "    if math.isnan(max(val_f1)):\n",
        "        optimal_f1s.append(max(val_f1[6:]))\n",
        "        idx = val_f1.index(optimal_f1s[-1])\n",
        "        precisions.append(val_p[idx])\n",
        "        recalls.append(val_r[idx])\n",
        "        accuracies.append(val_acc[idx])\n",
        "    else:\n",
        "        optimal_f1s.append(max(val_f1))\n",
        "        idx = val_f1.index(optimal_f1s[-1])\n",
        "        precisions.append(val_p[idx])\n",
        "        recalls.append(val_r[idx])\n",
        "        accuracies.append(val_acc[idx])\n",
        "\n",
        "\n",
        "print('F1 on MOH-X by 10-fold = ', optimal_f1s)\n",
        "print('Precision on MOH-X = ', np.mean(np.array(precisions)))\n",
        "print('Recall on MOH-X = ', np.mean(np.array(recalls)))\n",
        "print('F1 on MOH-X = ', np.mean(np.array(optimal_f1s)))\n",
        "print('Accuracy on MOH-X = ', np.mean(np.array(accuracies)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyuW9pMYP8uh",
        "outputId": "94e675bc-11c4-4f55-b165-031265a330de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Starting epoch 1------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/metaphor-detection/core/gao_files/sequence/util.py:218: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  eval_text = Variable(eval_text, volatile=True)\n",
            "/content/drive/MyDrive/metaphor-detection/core/gao_files/sequence/util.py:219: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  eval_lengths = Variable(eval_lengths, volatile=True)\n",
            "/content/drive/MyDrive/metaphor-detection/core/gao_files/sequence/util.py:220: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  eval_labels = Variable(eval_labels, volatile=True)\n",
            "/content/drive/MyDrive/metaphor-detection/core/gao_files/sequence/util.py:352: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  recall = 100 * grid[1, 1] / np.sum(grid[:, 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.55654101995566\n",
            "PRFA performance for  focus verb 40.0 100.0 57.142857142857146 40.0\n",
            "-----Starting epoch 2------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.77827050997783\n",
            "PRFA performance for  focus verb 40.0 100.0 57.142857142857146 40.0\n",
            "-----Starting epoch 3------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.77827050997783\n",
            "PRFA performance for  focus verb 40.0 100.0 57.142857142857146 40.0\n",
            "-----Starting epoch 4------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 40.0 100.0 57.142857142857146 40.0\n",
            "-----Starting epoch 5------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/metaphor-detection/core/gao_files/sequence/util.py:351: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  precision = 100 * grid[1, 1] / np.sum(grid[1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 40.0 100.0 57.142857142857146 40.0\n",
            "-----Starting epoch 6------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.77827050997783\n",
            "PRFA performance for  focus verb 40.0 100.0 57.142857142857146 40.0\n",
            "-----Starting epoch 7------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 40.0 100.0 57.142857142857146 40.0\n",
            "-----Starting epoch 8------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 40.0 100.0 57.142857142857146 40.0\n",
            "-----Starting epoch 9------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 40.0 100.0 57.142857142857146 40.0\n",
            "-----Starting epoch 10------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 40.0 100.0 57.142857142857146 40.0\n",
            "val_f1:  [57.142857142857146, 57.142857142857146, 57.142857142857146, 57.142857142857146, 57.142857142857146, 57.142857142857146, 57.142857142857146, 57.142857142857146, 57.142857142857146, 57.142857142857146]\n",
            "-----Starting epoch 1------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56236323851203\n",
            "PRFA performance for  focus verb 54.83870967741935 100.0 70.83333333333333 56.92307692307692\n",
            "-----Starting epoch 2------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.12472647702407\n",
            "PRFA performance for  focus verb 53.125 100.0 69.38775510204081 53.84615384615385\n",
            "-----Starting epoch 3------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56236323851203\n",
            "PRFA performance for  focus verb 53.96825396825397 100.0 70.10309278350515 55.38461538461539\n",
            "-----Starting epoch 4------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56236323851203\n",
            "PRFA performance for  focus verb 53.96825396825397 100.0 70.10309278350515 55.38461538461539\n",
            "-----Starting epoch 5------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56236323851203\n",
            "PRFA performance for  focus verb 53.96825396825397 100.0 70.10309278350515 55.38461538461539\n",
            "-----Starting epoch 6------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56236323851203\n",
            "PRFA performance for  focus verb 53.96825396825397 100.0 70.10309278350515 55.38461538461539\n",
            "-----Starting epoch 7------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56236323851203\n",
            "PRFA performance for  focus verb 53.96825396825397 100.0 70.10309278350515 55.38461538461539\n",
            "-----Starting epoch 8------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56236323851203\n",
            "PRFA performance for  focus verb 53.125 100.0 69.38775510204081 53.84615384615385\n",
            "-----Starting epoch 9------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56236323851203\n",
            "PRFA performance for  focus verb 53.125 100.0 69.38775510204081 53.84615384615385\n",
            "-----Starting epoch 10------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56236323851203\n",
            "PRFA performance for  focus verb 53.125 100.0 69.38775510204081 53.84615384615385\n",
            "val_f1:  [70.83333333333333, 69.38775510204081, 70.10309278350515, 70.10309278350515, 70.10309278350515, 70.10309278350515, 70.10309278350515, 69.38775510204081, 69.38775510204081, 69.38775510204081]\n",
            "-----Starting epoch 1------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.29078014184397\n",
            "PRFA performance for  focus verb 41.26984126984127 96.29629629629629 57.77777777777777 41.53846153846154\n",
            "-----Starting epoch 2------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76359338061465\n",
            "PRFA performance for  focus verb 40.32258064516129 92.5925925925926 56.17977528089887 40.0\n",
            "-----Starting epoch 3------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76359338061465\n",
            "PRFA performance for  focus verb 41.26984126984127 96.29629629629629 57.77777777777777 41.53846153846154\n",
            "-----Starting epoch 4------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76359338061465\n",
            "PRFA performance for  focus verb 41.53846153846154 100.0 58.69565217391305 41.53846153846154\n",
            "-----Starting epoch 5------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76359338061465\n",
            "PRFA performance for  focus verb 41.53846153846154 100.0 58.69565217391305 41.53846153846154\n",
            "-----Starting epoch 6------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76359338061465\n",
            "PRFA performance for  focus verb 39.682539682539684 92.5925925925926 55.55555555555556 38.46153846153846\n",
            "-----Starting epoch 7------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76359338061465\n",
            "PRFA performance for  focus verb 40.625 96.29629629629629 57.14285714285714 40.0\n",
            "-----Starting epoch 8------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76359338061465\n",
            "PRFA performance for  focus verb 40.625 96.29629629629629 57.14285714285714 40.0\n",
            "-----Starting epoch 9------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 40.625 96.29629629629629 57.14285714285714 40.0\n",
            "-----Starting epoch 10------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76359338061465\n",
            "PRFA performance for  focus verb 42.1875 100.0 59.34065934065934 43.07692307692308\n",
            "val_f1:  [57.77777777777777, 56.17977528089887, 57.77777777777777, 58.69565217391305, 58.69565217391305, 55.55555555555556, 57.14285714285714, 57.14285714285714, 57.14285714285714, 59.34065934065934]\n",
            "-----Starting epoch 1------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 50.0 96.96969696969697 65.97938144329896 49.23076923076923\n",
            "-----Starting epoch 2------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 51.5625 100.0 68.04123711340206 52.30769230769231\n",
            "-----Starting epoch 3------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.77426636568849\n",
            "PRFA performance for  focus verb 50.76923076923077 100.0 67.3469387755102 50.76923076923077\n",
            "-----Starting epoch 4------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 50.76923076923077 100.0 67.3469387755102 50.76923076923077\n",
            "-----Starting epoch 5------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 50.76923076923077 100.0 67.3469387755102 50.76923076923077\n",
            "-----Starting epoch 6------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 50.76923076923077 100.0 67.3469387755102 50.76923076923077\n",
            "-----Starting epoch 7------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 50.76923076923077 100.0 67.3469387755102 50.76923076923077\n",
            "-----Starting epoch 8------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 50.76923076923077 100.0 67.3469387755102 50.76923076923077\n",
            "-----Starting epoch 9------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 50.76923076923077 100.0 67.3469387755102 50.76923076923077\n",
            "-----Starting epoch 10------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 50.76923076923077 100.0 67.3469387755102 50.76923076923077\n",
            "val_f1:  [65.97938144329896, 68.04123711340206, 67.3469387755102, 67.3469387755102, 67.3469387755102, 67.3469387755102, 67.3469387755102, 67.3469387755102, 67.3469387755102, 67.3469387755102]\n",
            "-----Starting epoch 1------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.57537154989384\n",
            "PRFA performance for  focus verb 48.38709677419355 96.7741935483871 64.51612903225806 49.23076923076923\n",
            "-----Starting epoch 2------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.57537154989384\n",
            "PRFA performance for  focus verb 48.4375 100.0 65.26315789473684 49.23076923076923\n",
            "-----Starting epoch 3------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 49.20634920634921 100.0 65.95744680851064 50.76923076923077\n",
            "-----Starting epoch 4------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78768577494692\n",
            "PRFA performance for  focus verb 46.774193548387096 93.54838709677419 62.365591397849464 46.15384615384615\n",
            "-----Starting epoch 5------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 47.69230769230769 100.0 64.58333333333334 47.69230769230769\n",
            "-----Starting epoch 6------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78768577494692\n",
            "PRFA performance for  focus verb 47.61904761904762 96.7741935483871 63.82978723404255 47.69230769230769\n",
            "-----Starting epoch 7------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78768577494692\n",
            "PRFA performance for  focus verb 47.61904761904762 96.7741935483871 63.82978723404255 47.69230769230769\n",
            "-----Starting epoch 8------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78768577494692\n",
            "PRFA performance for  focus verb 48.4375 100.0 65.26315789473684 49.23076923076923\n",
            "-----Starting epoch 9------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78768577494692\n",
            "PRFA performance for  focus verb 48.4375 100.0 65.26315789473684 49.23076923076923\n",
            "-----Starting epoch 10------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78768577494692\n",
            "PRFA performance for  focus verb 48.4375 100.0 65.26315789473684 49.23076923076923\n",
            "val_f1:  [64.51612903225806, 65.26315789473684, 65.95744680851064, 62.365591397849464, 64.58333333333334, 63.82978723404255, 63.82978723404255, 65.26315789473684, 65.26315789473684, 65.26315789473684]\n",
            "-----Starting epoch 1------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 98.76796714579055\n",
            "PRFA performance for  focus verb 58.333333333333336 87.5 70.0 53.84615384615385\n",
            "-----Starting epoch 2------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.58932238193019\n",
            "PRFA performance for  focus verb 59.67741935483871 92.5 72.54901960784314 56.92307692307692\n",
            "-----Starting epoch 3------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.58932238193019\n",
            "PRFA performance for  focus verb 59.01639344262295 90.0 71.28712871287128 55.38461538461539\n",
            "-----Starting epoch 4------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.38398357289527\n",
            "PRFA performance for  focus verb 59.67741935483871 92.5 72.54901960784314 56.92307692307692\n",
            "-----Starting epoch 5------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.58932238193019\n",
            "PRFA performance for  focus verb 60.317460317460316 95.0 73.7864077669903 58.46153846153846\n",
            "-----Starting epoch 6------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.58932238193019\n",
            "PRFA performance for  focus verb 60.317460317460316 95.0 73.7864077669903 58.46153846153846\n",
            "-----Starting epoch 7------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.58932238193019\n",
            "PRFA performance for  focus verb 60.317460317460316 95.0 73.7864077669903 58.46153846153846\n",
            "-----Starting epoch 8------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.58932238193019\n",
            "PRFA performance for  focus verb 60.317460317460316 95.0 73.7864077669903 58.46153846153846\n",
            "-----Starting epoch 9------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.58932238193019\n",
            "PRFA performance for  focus verb 60.317460317460316 95.0 73.7864077669903 58.46153846153846\n",
            "-----Starting epoch 10------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.58932238193019\n",
            "PRFA performance for  focus verb 60.317460317460316 95.0 73.7864077669903 58.46153846153846\n",
            "val_f1:  [70.0, 72.54901960784314, 71.28712871287128, 72.54901960784314, 73.7864077669903, 73.7864077669903, 73.7864077669903, 73.7864077669903, 73.7864077669903, 73.7864077669903]\n",
            "-----Starting epoch 1------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 50.79365079365079 100.0 67.36842105263159 52.30769230769231\n",
            "-----Starting epoch 2------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56896551724138\n",
            "PRFA performance for  focus verb 49.20634920634921 96.875 65.26315789473684 49.23076923076923\n",
            "-----Starting epoch 3------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78448275862068\n",
            "PRFA performance for  focus verb 48.4375 96.875 64.58333333333333 47.69230769230769\n",
            "-----Starting epoch 4------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56896551724138\n",
            "PRFA performance for  focus verb 48.4375 96.875 64.58333333333333 47.69230769230769\n",
            "-----Starting epoch 5------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78448275862068\n",
            "PRFA performance for  focus verb 48.4375 96.875 64.58333333333333 47.69230769230769\n",
            "-----Starting epoch 6------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78448275862068\n",
            "PRFA performance for  focus verb 48.4375 96.875 64.58333333333333 47.69230769230769\n",
            "-----Starting epoch 7------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56896551724138\n",
            "PRFA performance for  focus verb 48.4375 96.875 64.58333333333333 47.69230769230769\n",
            "-----Starting epoch 8------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 48.4375 96.875 64.58333333333333 47.69230769230769\n",
            "-----Starting epoch 9------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 48.4375 96.875 64.58333333333333 47.69230769230769\n",
            "-----Starting epoch 10------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78448275862068\n",
            "PRFA performance for  focus verb 48.4375 96.875 64.58333333333333 47.69230769230769\n",
            "val_f1:  [67.36842105263159, 65.26315789473684, 64.58333333333333, 64.58333333333333, 64.58333333333333, 64.58333333333333, 64.58333333333333, 64.58333333333333, 64.58333333333333, 64.58333333333333]\n",
            "-----Starting epoch 1------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78632478632478\n",
            "PRFA performance for  focus verb 57.377049180327866 97.22222222222223 72.16494845360825 58.46153846153846\n",
            "-----Starting epoch 2------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78632478632478\n",
            "PRFA performance for  focus verb 55.38461538461539 100.0 71.2871287128713 55.38461538461539\n",
            "-----Starting epoch 3------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78632478632478\n",
            "PRFA performance for  focus verb 55.38461538461539 100.0 71.2871287128713 55.38461538461539\n",
            "-----Starting epoch 4------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78632478632478\n",
            "PRFA performance for  focus verb 56.25 100.0 72.0 56.92307692307692\n",
            "-----Starting epoch 5------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78632478632478\n",
            "PRFA performance for  focus verb 56.25 100.0 72.0 56.92307692307692\n",
            "-----Starting epoch 6------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78632478632478\n",
            "PRFA performance for  focus verb 56.25 100.0 72.0 56.92307692307692\n",
            "-----Starting epoch 7------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78632478632478\n",
            "PRFA performance for  focus verb 56.25 100.0 72.0 56.92307692307692\n",
            "-----Starting epoch 8------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78632478632478\n",
            "PRFA performance for  focus verb 56.25 100.0 72.0 56.92307692307692\n",
            "-----Starting epoch 9------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78632478632478\n",
            "PRFA performance for  focus verb 56.25 100.0 72.0 56.92307692307692\n",
            "-----Starting epoch 10------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78632478632478\n",
            "PRFA performance for  focus verb 56.25 100.0 72.0 56.92307692307692\n",
            "val_f1:  [72.16494845360825, 71.2871287128713, 71.2871287128713, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0]\n",
            "-----Starting epoch 1------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.53488372093024\n",
            "PRFA performance for  focus verb 46.15384615384615 100.0 63.1578947368421 46.15384615384615\n",
            "-----Starting epoch 2------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76744186046511\n",
            "PRFA performance for  focus verb 46.875 100.0 63.829787234042556 47.69230769230769\n",
            "-----Starting epoch 3------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76744186046511\n",
            "PRFA performance for  focus verb 46.875 100.0 63.829787234042556 47.69230769230769\n",
            "-----Starting epoch 4------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76744186046511\n",
            "PRFA performance for  focus verb 46.875 100.0 63.829787234042556 47.69230769230769\n",
            "-----Starting epoch 5------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76744186046511\n",
            "PRFA performance for  focus verb 46.875 100.0 63.829787234042556 47.69230769230769\n",
            "-----Starting epoch 6------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.53488372093024\n",
            "PRFA performance for  focus verb 46.875 100.0 63.829787234042556 47.69230769230769\n",
            "-----Starting epoch 7------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76744186046511\n",
            "PRFA performance for  focus verb 46.875 100.0 63.829787234042556 47.69230769230769\n",
            "-----Starting epoch 8------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76744186046511\n",
            "PRFA performance for  focus verb 46.875 100.0 63.829787234042556 47.69230769230769\n",
            "-----Starting epoch 9------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76744186046511\n",
            "PRFA performance for  focus verb 46.875 100.0 63.829787234042556 47.69230769230769\n",
            "-----Starting epoch 10------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.76744186046511\n",
            "PRFA performance for  focus verb 46.875 100.0 63.829787234042556 47.69230769230769\n",
            "val_f1:  [63.1578947368421, 63.829787234042556, 63.829787234042556, 63.829787234042556, 63.829787234042556, 63.829787234042556, 63.829787234042556, 63.829787234042556, 63.829787234042556, 63.829787234042556]\n",
            "-----Starting epoch 1------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.08045977011494\n",
            "PRFA performance for  focus verb 45.6140350877193 100.0 62.650602409638566 50.0\n",
            "-----Starting epoch 2------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.08045977011494\n",
            "PRFA performance for  focus verb 43.333333333333336 100.0 60.465116279069775 45.16129032258065\n",
            "-----Starting epoch 3------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.3103448275862\n",
            "PRFA performance for  focus verb 45.6140350877193 100.0 62.650602409638566 50.0\n",
            "-----Starting epoch 4------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.3103448275862\n",
            "PRFA performance for  focus verb 44.827586206896555 100.0 61.904761904761905 48.38709677419355\n",
            "-----Starting epoch 5------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.3103448275862\n",
            "PRFA performance for  focus verb 44.827586206896555 100.0 61.904761904761905 48.38709677419355\n",
            "-----Starting epoch 6------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.3103448275862\n",
            "PRFA performance for  focus verb 44.827586206896555 100.0 61.904761904761905 48.38709677419355\n",
            "-----Starting epoch 7------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.3103448275862\n",
            "PRFA performance for  focus verb 43.333333333333336 100.0 60.465116279069775 45.16129032258065\n",
            "-----Starting epoch 8------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.3103448275862\n",
            "PRFA performance for  focus verb 43.333333333333336 100.0 60.465116279069775 45.16129032258065\n",
            "-----Starting epoch 9------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.3103448275862\n",
            "PRFA performance for  focus verb 43.333333333333336 100.0 60.465116279069775 45.16129032258065\n",
            "-----Starting epoch 10------\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.3103448275862\n",
            "PRFA performance for  focus verb 45.6140350877193 100.0 62.650602409638566 50.0\n",
            "val_f1:  [62.650602409638566, 60.465116279069775, 62.650602409638566, 61.904761904761905, 61.904761904761905, 61.904761904761905, 60.465116279069775, 60.465116279069775, 60.465116279069775, 62.650602409638566]\n",
            "F1 on MOH-X by 10-fold =  [57.142857142857146, 70.83333333333333, 59.34065934065934, 68.04123711340206, 65.95744680851064, 73.7864077669903, 67.36842105263159, 72.16494845360825, 63.829787234042556, 62.650602409638566]\n",
            "Precision on MOH-X =  49.87722542629268\n",
            "Recall on MOH-X =  99.22222222222221\n",
            "F1 on MOH-X =  66.11157006556736\n",
            "Accuracy on MOH-X =  51.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "gao_scores = [79.1, 73.5, 75.6, 77.2]\n",
        "our_scores = [np.mean(np.array(precisions)),\n",
        "  np.mean(np.array(recalls)),\n",
        "  np.mean(np.array(optimal_f1s)),\n",
        "  np.mean(np.array(accuracies))]\n",
        "our_scores = [round(score,1) for score in our_scores]\n",
        "all_scores = [gao_scores, our_scores]\n",
        "all_scores_df = pd.DataFrame(all_scores, columns= ['P', 'R', 'F1', 'Acc'], index=['Gao et al', 'US'])\n",
        "print(\"Moh-X seq model: classification task\\n\")\n",
        "all_scores_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "FpkmTfI_VlHY",
        "outputId": "2cbab653-d7b4-4058-cda8-36b5623f0fb8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moh-X seq model: classification task\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c2215768-cacc-4ca8-978a-c8b554f8f4f7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>P</th>\n",
              "      <th>R</th>\n",
              "      <th>F1</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Gao et al</th>\n",
              "      <td>79.1</td>\n",
              "      <td>73.5</td>\n",
              "      <td>75.6</td>\n",
              "      <td>77.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>US</th>\n",
              "      <td>49.9</td>\n",
              "      <td>99.2</td>\n",
              "      <td>66.1</td>\n",
              "      <td>51.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2215768-cacc-4ca8-978a-c8b554f8f4f7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c2215768-cacc-4ca8-978a-c8b554f8f4f7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c2215768-cacc-4ca8-978a-c8b554f8f4f7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              P     R    F1   Acc\n",
              "Gao et al  79.1  73.5  75.6  77.2\n",
              "US         49.9  99.2  66.1  51.0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UFFpyvZ6dSAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}