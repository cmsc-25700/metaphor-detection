{"cells":[{"cell_type":"markdown","source":["## Code for replicating gao et al research on TroFi Sequence Model"],"metadata":{"id":"e3nE-xGdOaXq"},"id":"e3nE-xGdOaXq"},{"cell_type":"code","execution_count":1,"id":"e8a79e9d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8a79e9d","executionInfo":{"status":"ok","timestamp":1646360987651,"user_tz":360,"elapsed":754,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"7177b855-6c8c-4df6-9f06-c5296d94eb5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# mount drive\n","from google.colab import drive\n","ROOT = '/content/drive'\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# add repo directory to path\n","import os\n","import sys\n","from os.path import join \n","repo_dir = '/content/drive/MyDrive/Repos/metaphor-detection'\n","if repo_dir not in sys.path:\n","    sys.path.append(repo_dir)\n","print(sys.path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-ZJKsRtBhQG","executionInfo":{"status":"ok","timestamp":1646360987651,"user_tz":360,"elapsed":6,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"d98b9e81-f426-4e34-ce75-611ef20dcdea"},"id":"1-ZJKsRtBhQG","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython', '/content/drive/MyDrive/Repos/metaphor-detection']\n"]}]},{"cell_type":"code","source":["# directories\n","# to download glove and elmo vectors see: notebooks/Download_large_data.ipynb\n","data_dir = repo_dir + '/resources/metaphor-in-context/data/'\n","glove_dir = repo_dir + '/resources/glove/'\n","elmo_dir = repo_dir + '/resources/elmo/'"],"metadata":{"id":"SbcZ3uaEJnc4","executionInfo":{"status":"ok","timestamp":1646360987652,"user_tz":360,"elapsed":4,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}}},"id":"SbcZ3uaEJnc4","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Gao Code"],"metadata":{"id":"9Gf7J_dTO1wl"},"id":"9Gf7J_dTO1wl"},{"cell_type":"code","source":["# pip install requirements (takes a while)\n","!cd drive/MyDrive/Repos/metaphor-detection/; pip install -r gao-g-requirements.txt\n","!pip install --upgrade google-cloud-storage"],"metadata":{"id":"K2km4tJhCQa5"},"id":"K2km4tJhCQa5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from core.gao_files.sequence.util import get_num_lines, get_pos2idx_idx2pos, index_sequence, get_vocab, embed_indexed_sequence, \\\n","    get_word2idx_idx2word, get_embedding_matrix, write_predictions, get_performance_VUAverb_val\n","from core.gao_files.sequence.util import TextDatasetWithGloveElmoSuffix as TextDataset\n","from core.gao_files.sequence.util import evaluate\n","from core.gao_files.sequence.model import RNNSequenceModel\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","\n","import csv\n","import h5py\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","import random"],"metadata":{"id":"KZtrvYTOIEtT","executionInfo":{"status":"ok","timestamp":1646361010311,"user_tz":360,"elapsed":7746,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}}},"id":"KZtrvYTOIEtT","execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(\"PyTorch version:\")\n","print(torch.__version__)\n","print(\"GPU Detected:\")\n","print(torch.cuda.is_available())\n","using_GPU = torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ZzhhVmlIbgs","executionInfo":{"status":"ok","timestamp":1646361010311,"user_tz":360,"elapsed":16,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"83d56236-d4dd-4b83-cab3-c359e4d19ea2"},"id":"5ZzhhVmlIbgs","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version:\n","1.10.0+cu111\n","GPU Detected:\n","True\n"]}]},{"cell_type":"code","source":["\"\"\"\n","1. Data pre-processing\n","\"\"\"\n","\n","'''\n","1.2 TroFi\n","get raw dataset as a list:\n","  Each element is a triple:\n","    a sentence: string\n","    a index: int: idx of the focus verb\n","    a label: int 1 or 0\n","'''\n","raw_trofi = []\n","\n","with open(data_dir +'TroFi/TroFi_formatted_all3737.csv') as f:\n","    lines = csv.reader(f)\n","    next(lines)\n","    for line in lines:\n","        sentence = line[1]\n","        label_seq = [0] * len(sentence.split())\n","        pos_seq = [0] * len(label_seq)\n","        verb_idx = int(line[2])\n","        verb_label = int(line[3])\n","        label_seq[verb_idx] = verb_label\n","        pos_seq[verb_idx] = 1   # idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n","        raw_trofi.append([sentence.strip(), label_seq, pos_seq])\n","\n","\n","print('TroFi dataset division: ', len(raw_trofi))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"an6thZhFIlMx","executionInfo":{"status":"ok","timestamp":1646361010406,"user_tz":360,"elapsed":6,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"6d2037f8-6a39-47bd-c25f-466969d6dee9"},"id":"an6thZhFIlMx","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["TroFi dataset division:  3737\n"]}]},{"cell_type":"code","source":["\"\"\"\n","2. Data preparation\n","\"\"\"\n","'''\n","2. 1\n","get vocabulary and glove embeddings in raw dataset \n","'''\n","# vocab is a set of words\n","vocab = get_vocab(raw_trofi)\n","# two dictionaries. <PAD>: 0, <UNK>: 1\n","word2idx, idx2word = get_word2idx_idx2word(vocab)\n","# glove_embeddings a nn.Embeddings\n","glove_embeddings = get_embedding_matrix(glove_dir + 'glove.840B.300d.txt', word2idx, idx2word, normalization=False)\n","# elmo_embeddings\n","# set elmos_trofi=None to exclude elmo vectors. Also need to change the embedding_dim in later model initialization\n","elmos_trofi = h5py.File(elmo_dir + 'TroFi3737.hdf5', 'r')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9kjUIe6NJwD9","executionInfo":{"status":"ok","timestamp":1646361094938,"user_tz":360,"elapsed":84537,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"29d509be-c4d8-42cb-afed-58d5023a52db"},"id":"9kjUIe6NJwD9","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab size:  14881\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2196017/2196017 [00:59<00:00, 36717.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Number of pre-trained word vectors loaded:  14674\n","Embeddings mean:  0.0016923490911722183\n","Embeddings stdev:  0.3751399517059326\n"]}]},{"cell_type":"code","source":["'''\n","2. 2\n","embed the datasets\n","'''\n","random.seed(0)\n","random.shuffle(raw_trofi)\n","\n","# second argument is the post sequence, which we don't need\n","embedded_trofi = [[embed_indexed_sequence(example[0], example[2], word2idx,\n","                                      glove_embeddings, elmos_trofi, None),\n","                       example[2], example[1]]\n","                      for example in raw_trofi]"],"metadata":{"id":"HYsVbvgRc1o6","executionInfo":{"status":"ok","timestamp":1646361102516,"user_tz":360,"elapsed":7593,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}}},"id":"HYsVbvgRc1o6","execution_count":9,"outputs":[]},{"cell_type":"code","source":["'''\n","2. 3 10-fold cross validation\n","'''\n","# separate the embedded_sentences and labels into 2 list, in order to pass into the TextDataset as argument\n","sentences = [example[0] for example in embedded_trofi]\n","poss = [example[1] for example in embedded_trofi]\n","labels = [example[2] for example in embedded_trofi]\n","# ten_folds is a list of 10 tuples, each tuple is (list_of_embedded_sentences, list_of_corresponding_labels)\n","ten_folds = []\n","fold_size = int(3737 / 10)\n","for i in range(10):\n","    ten_folds.append((sentences[i * fold_size:(i + 1) * fold_size],\n","                      poss[i * fold_size:(i + 1) * fold_size],\n","                      labels[i * fold_size:(i + 1) * fold_size]))\n","\n","idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n","\n","optimal_f1s = []\n","optimal_ps = []\n","optimal_rs = []\n","optimal_accs = []\n","predictions_all = []\n","for i in range(10):\n","    '''\n","    2. 3\n","    set up Dataloader for batching\n","    '''\n","    training_sentences = []\n","    training_labels = []\n","    training_poss = []\n","    for j in range(10):\n","        if j != i:\n","            training_sentences.extend(ten_folds[j][0])\n","            training_poss.extend(ten_folds[j][1])\n","            training_labels.extend(ten_folds[j][2])\n","    training_dataset_trofi = TextDataset(training_sentences, training_poss, training_labels)\n","    val_dataset_trofi = TextDataset(ten_folds[i][0], ten_folds[i][1], ten_folds[i][2])\n","\n","    # Data-related hyperparameters\n","    batch_size = 10\n","    # Set up a DataLoader for the training, validation, and test dataset\n","    train_dataloader_trofi = DataLoader(dataset=training_dataset_trofi, batch_size=batch_size, shuffle=True,\n","                                        collate_fn=TextDataset.collate_fn)\n","    val_dataloader_trofi = DataLoader(dataset=val_dataset_trofi, batch_size=batch_size, shuffle=False,\n","                                      collate_fn=TextDataset.collate_fn)\n","    \"\"\"\n","    3. Model training\n","    \"\"\"\n","    '''\n","    3. 1 \n","    set up model, loss criterion, optimizer\n","    '''\n","    # Instantiate the model\n","    # embedding_dim = glove + elmo + suffix indicator\n","    # dropout1: dropout on input to RNN\n","    # dropout2: dropout in RNN; would be used if num_layers=1\n","    # dropout3: dropout on hidden state of RNN to linear layer\n","    RNNseq_model = RNNSequenceModel(num_classes=2, embedding_dim=300+1024, hidden_size=300,\n","                                    num_layers=1, bidir=True,\n","                                    dropout1=0.5, dropout2=0, dropout3=0.2)\n","    # Move the model to the GPU if available\n","    if using_GPU:\n","        RNNseq_model = RNNseq_model.cuda()\n","    # Set up criterion for calculating loss\n","    loss_criterion = nn.NLLLoss()\n","    # Set up an optimizer for updating the parameters of the rnn_clf\n","    rnn_optimizer = optim.Adam(RNNseq_model.parameters(), lr=0.001)\n","    # Number of epochs (passes through the dataset) to train the model for.\n","    num_epochs = 10\n","\n","    '''\n","    3. 2\n","    train model\n","    '''\n","    train_loss = []\n","    val_loss = []\n","    performance_matrix = None\n","    val_f1 = []\n","    val_p = []\n","    val_r = []\n","    val_acc = []\n","    train_f1 = []\n","    # A counter for the number of gradient updates\n","    num_iter = 0\n","    model_index = 0\n","    comparable = []\n","    for epoch in range(num_epochs):\n","        print(\"Starting epoch {}\".format(epoch + 1))\n","        for (__, example_text, example_lengths, labels) in train_dataloader_trofi:\n","            example_text = Variable(example_text)\n","            example_lengths = Variable(example_lengths)\n","            labels = Variable(labels)\n","            if using_GPU:\n","                example_text = example_text.cuda()\n","                example_lengths = example_lengths.cuda()\n","                labels = labels.cuda()\n","            # predicted shape: (batch_size, seq_len, 2)\n","            predicted = RNNseq_model(example_text, example_lengths)\n","            batch_loss = loss_criterion(predicted.view(-1, 2), labels.view(-1))\n","            rnn_optimizer.zero_grad()\n","            batch_loss.backward()\n","            rnn_optimizer.step()\n","            num_iter += 1\n","            # Calculate validation and training set loss and accuracy every 200 gradient updates\n","            if num_iter % 200 == 0:\n","                avg_eval_loss, performance_matrix = evaluate(idx2pos, val_dataloader_trofi, RNNseq_model,\n","                                                             loss_criterion, using_GPU)\n","                val_loss.append(avg_eval_loss)\n","                val_p.append(performance_matrix[1][0])\n","                val_r.append(performance_matrix[1][1])\n","                val_f1.append(performance_matrix[1][2])\n","                val_acc.append(performance_matrix[1][3])\n","                print(\"Iteration {}. Validation Loss {}.\".format(num_iter, avg_eval_loss))\n","#                 avg_eval_loss, performance_matrix = evaluate(idx2pos, train_dataloader_trofi, RNNseq_model,\n","#                                                              loss_criterion, using_GPU)\n","#                 train_loss.append(avg_eval_loss)\n","#                 train_f1.append(performance_matrix[1][1])\n","#                 print(\"Iteration {}. Training Loss {}.\".format(num_iter, avg_eval_loss))\n","    print(\"Training done for fold {}\".format(i))\n","\n","    \"\"\"\n","    3.3\n","    plot the training process: MET F1 and losses for validation and training dataset\n","    \"\"\"\n","#     plt.figure(0)\n","#     plt.title('F1 for TroFI dataset on fold ' + str(i))\n","#     plt.xlabel('iteration (unit:200)')\n","#     plt.ylabel('F1')\n","#     plt.plot(val_f1, 'g')\n","#     #     plt.plot(train_f1, 'b')\n","#     plt.legend(['Validation F1', 'Training F1'], loc='upper right')\n","#     plt.show()\n","\n","#     plt.figure(1)\n","#     plt.title('Loss for TroFi dataset on fold ' + str(i))\n","#     plt.xlabel('iteration (unit:200)')\n","#     plt.ylabel('Loss')\n","#     plt.plot(val_loss, 'g')\n","#     #     plt.plot(train_loss, 'b')\n","#     plt.legend(['Validation loss', 'Training loss'], loc='upper right')\n","#     plt.show()\n","\n","    \"\"\"\n","    store the best f1\n","    \"\"\"\n","    print('val_f1: ', val_f1)\n","    idx = 0\n","    if math.isnan(max(val_f1)):\n","        optimal_f1s.append(max(val_f1[6:]))\n","        idx = val_f1.index(optimal_f1s[-1])\n","        optimal_ps.append(val_p[idx])\n","        optimal_rs.append(val_r[idx])\n","        optimal_accs.append(val_acc[idx])\n","    else:\n","        optimal_f1s.append(max(val_f1))\n","        idx = val_f1.index(optimal_f1s[-1])\n","        optimal_ps.append(val_p[idx])\n","        optimal_rs.append(val_r[idx])\n","        optimal_accs.append(val_acc[idx])\n","\n","\n","\"\"\"\n","print out the performance\n","plot the performance on each fold\n","\"\"\"\n","print('F1 on TroFi by 10-fold = ', optimal_f1s)\n","print('Precision on TroFi = ', np.mean(np.array(optimal_ps)))\n","print('Recall on TroFi = ', np.mean(np.array(optimal_rs)))\n","print('F1 on TroFi = ', np.mean(np.array(optimal_f1s)))\n","print('Accuracy on TroFi = ', np.mean(np.array(optimal_accs)))"],"metadata":{"id":"XXxTTXbwpcNy"},"id":"XXxTTXbwpcNy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('F1 on TroFi by 10-fold = ', optimal_f1s)\n","print('Precision on TroFi = ', np.mean(np.array(optimal_ps)))\n","print('Recall on TroFi = ', np.mean(np.array(optimal_rs)))\n","print('F1 on TroFi = ', np.mean(np.array(optimal_f1s)))\n","print('Accuracy on TroFi = ', np.mean(np.array(optimal_accs)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TU5vw3n91R5_","executionInfo":{"status":"ok","timestamp":1646362190804,"user_tz":360,"elapsed":122,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"48a89c24-b67e-4a44-88fd-1521b7d57c7e"},"id":"TU5vw3n91R5_","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 on TroFi by 10-fold =  [74.82993197278911, 71.25, 68.96551724137932, 68.24925816023739, 68.85245901639344, 70.93023255813954, 70.15384615384616, 72.38095238095237, 71.9242902208202, 71.21661721068249]\n","Precision on TroFi =  70.44295206262248\n","Recall on TroFi =  71.46397164548841\n","F1 on TroFi =  70.875310491524\n","Accuracy on TroFi =  74.36997319034852\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"colab":{"name":"replicate_trofi_seq_model_for_classification.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}