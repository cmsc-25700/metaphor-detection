{"cells":[{"cell_type":"markdown","source":["## Code for replicating gao et al research on TroFi Sequence Model"],"metadata":{"id":"e3nE-xGdOaXq"},"id":"e3nE-xGdOaXq"},{"cell_type":"code","execution_count":1,"id":"e8a79e9d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e8a79e9d","executionInfo":{"status":"ok","timestamp":1646360987651,"user_tz":360,"elapsed":754,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"7177b855-6c8c-4df6-9f06-c5296d94eb5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# mount drive\n","from google.colab import drive\n","ROOT = '/content/drive'\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# add repo directory to path\n","import os\n","import sys\n","from os.path import join \n","repo_dir = '/content/drive/MyDrive/Repos/metaphor-detection'\n","if repo_dir not in sys.path:\n","    sys.path.append(repo_dir)\n","print(sys.path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-ZJKsRtBhQG","executionInfo":{"status":"ok","timestamp":1646360987651,"user_tz":360,"elapsed":6,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"d98b9e81-f426-4e34-ce75-611ef20dcdea"},"id":"1-ZJKsRtBhQG","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython', '/content/drive/MyDrive/Repos/metaphor-detection']\n"]}]},{"cell_type":"code","source":["# directories\n","# to download glove and elmo vectors see: notebooks/Download_large_data.ipynb\n","data_dir = repo_dir + '/resources/metaphor-in-context/data/'\n","glove_dir = repo_dir + '/resources/glove/'\n","elmo_dir = repo_dir + '/resources/elmo/'"],"metadata":{"id":"SbcZ3uaEJnc4","executionInfo":{"status":"ok","timestamp":1646360987652,"user_tz":360,"elapsed":4,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}}},"id":"SbcZ3uaEJnc4","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Gao Code"],"metadata":{"id":"9Gf7J_dTO1wl"},"id":"9Gf7J_dTO1wl"},{"cell_type":"code","source":["# pip install requirements (takes a while)\n","!cd drive/MyDrive/Repos/metaphor-detection/; pip install -r gao-g-requirements.txt\n","!pip install --upgrade google-cloud-storage"],"metadata":{"id":"K2km4tJhCQa5"},"id":"K2km4tJhCQa5","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"kEhCwA-c3YoA"},"id":"kEhCwA-c3YoA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from core.gao_files.sequence.util import get_num_lines, get_pos2idx_idx2pos, index_sequence, get_vocab, embed_indexed_sequence, \\\n","    get_word2idx_idx2word, get_embedding_matrix, write_predictions, get_performance_VUAverb_val\n","from core.gao_files.sequence.util import TextDatasetWithGloveElmoSuffix as TextDataset\n","from core.gao_files.sequence.util import evaluate\n","from core.gao_files.sequence.model import RNNSequenceModel\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","\n","import csv\n","import h5py\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","import random"],"metadata":{"id":"KZtrvYTOIEtT","executionInfo":{"status":"ok","timestamp":1646361010311,"user_tz":360,"elapsed":7746,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}}},"id":"KZtrvYTOIEtT","execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(\"PyTorch version:\")\n","print(torch.__version__)\n","print(\"GPU Detected:\")\n","print(torch.cuda.is_available())\n","using_GPU = torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ZzhhVmlIbgs","executionInfo":{"status":"ok","timestamp":1646361010311,"user_tz":360,"elapsed":16,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"83d56236-d4dd-4b83-cab3-c359e4d19ea2"},"id":"5ZzhhVmlIbgs","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version:\n","1.10.0+cu111\n","GPU Detected:\n","True\n"]}]},{"cell_type":"code","source":["\"\"\"\n","1. Data pre-processing\n","\"\"\"\n","\n","'''\n","1.2 TroFi\n","get raw dataset as a list:\n","  Each element is a triple:\n","    a sentence: string\n","    a index: int: idx of the focus verb\n","    a label: int 1 or 0\n","'''\n","raw_trofi = []\n","\n","with open(data_dir +'TroFi/TroFi_formatted_all3737.csv') as f:\n","    lines = csv.reader(f)\n","    next(lines)\n","    for line in lines:\n","        sentence = line[1]\n","        label_seq = [0] * len(sentence.split())\n","        pos_seq = [0] * len(label_seq)\n","        verb_idx = int(line[2])\n","        verb_label = int(line[3])\n","        label_seq[verb_idx] = verb_label\n","        pos_seq[verb_idx] = 1   # idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n","        raw_trofi.append([sentence.strip(), label_seq, pos_seq])\n","\n","\n","print('TroFi dataset division: ', len(raw_trofi))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"an6thZhFIlMx","executionInfo":{"status":"ok","timestamp":1646361010406,"user_tz":360,"elapsed":6,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"6d2037f8-6a39-47bd-c25f-466969d6dee9"},"id":"an6thZhFIlMx","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["TroFi dataset division:  3737\n"]}]},{"cell_type":"code","source":["\"\"\"\n","2. Data preparation\n","\"\"\"\n","'''\n","2. 1\n","get vocabulary and glove embeddings in raw dataset \n","'''\n","# vocab is a set of words\n","vocab = get_vocab(raw_trofi)\n","# two dictionaries. <PAD>: 0, <UNK>: 1\n","word2idx, idx2word = get_word2idx_idx2word(vocab)\n","# glove_embeddings a nn.Embeddings\n","glove_embeddings = get_embedding_matrix(glove_dir + 'glove.840B.300d.txt', word2idx, idx2word, normalization=False)\n","# elmo_embeddings\n","# set elmos_trofi=None to exclude elmo vectors. Also need to change the embedding_dim in later model initialization\n","elmos_trofi = h5py.File(elmo_dir + 'TroFi3737.hdf5', 'r')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9kjUIe6NJwD9","executionInfo":{"status":"ok","timestamp":1646361094938,"user_tz":360,"elapsed":84537,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"29d509be-c4d8-42cb-afed-58d5023a52db"},"id":"9kjUIe6NJwD9","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab size:  14881\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2196017/2196017 [00:59<00:00, 36717.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Number of pre-trained word vectors loaded:  14674\n","Embeddings mean:  0.0016923490911722183\n","Embeddings stdev:  0.3751399517059326\n"]}]},{"cell_type":"code","source":["'''\n","2. 2\n","embed the datasets\n","'''\n","random.seed(0)\n","random.shuffle(raw_trofi)\n","\n","# second argument is the post sequence, which we don't need\n","embedded_trofi = [[embed_indexed_sequence(example[0], example[2], word2idx,\n","                                      glove_embeddings, elmos_trofi, None),\n","                       example[2], example[1]]\n","                      for example in raw_trofi]"],"metadata":{"id":"HYsVbvgRc1o6","executionInfo":{"status":"ok","timestamp":1646361102516,"user_tz":360,"elapsed":7593,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}}},"id":"HYsVbvgRc1o6","execution_count":9,"outputs":[]},{"cell_type":"code","source":["'''\n","2. 3 10-fold cross validation\n","'''\n","# separate the embedded_sentences and labels into 2 list, in order to pass into the TextDataset as argument\n","sentences = [example[0] for example in embedded_trofi]\n","poss = [example[1] for example in embedded_trofi]\n","labels = [example[2] for example in embedded_trofi]\n","# ten_folds is a list of 10 tuples, each tuple is (list_of_embedded_sentences, list_of_corresponding_labels)\n","ten_folds = []\n","fold_size = int(3737 / 10)\n","for i in range(10):\n","    ten_folds.append((sentences[i * fold_size:(i + 1) * fold_size],\n","                      poss[i * fold_size:(i + 1) * fold_size],\n","                      labels[i * fold_size:(i + 1) * fold_size]))\n","\n","idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n","\n","optimal_f1s = []\n","optimal_ps = []\n","optimal_rs = []\n","optimal_accs = []\n","predictions_all = []\n","for i in range(10):\n","    '''\n","    2. 3\n","    set up Dataloader for batching\n","    '''\n","    training_sentences = []\n","    training_labels = []\n","    training_poss = []\n","    for j in range(10):\n","        if j != i:\n","            training_sentences.extend(ten_folds[j][0])\n","            training_poss.extend(ten_folds[j][1])\n","            training_labels.extend(ten_folds[j][2])\n","    training_dataset_trofi = TextDataset(training_sentences, training_poss, training_labels)\n","    val_dataset_trofi = TextDataset(ten_folds[i][0], ten_folds[i][1], ten_folds[i][2])\n","\n","    # Data-related hyperparameters\n","    batch_size = 10\n","    # Set up a DataLoader for the training, validation, and test dataset\n","    train_dataloader_trofi = DataLoader(dataset=training_dataset_trofi, batch_size=batch_size, shuffle=True,\n","                                        collate_fn=TextDataset.collate_fn)\n","    val_dataloader_trofi = DataLoader(dataset=val_dataset_trofi, batch_size=batch_size, shuffle=False,\n","                                      collate_fn=TextDataset.collate_fn)\n","    \"\"\"\n","    3. Model training\n","    \"\"\"\n","    '''\n","    3. 1 \n","    set up model, loss criterion, optimizer\n","    '''\n","    # Instantiate the model\n","    # embedding_dim = glove + elmo + suffix indicator\n","    # dropout1: dropout on input to RNN\n","    # dropout2: dropout in RNN; would be used if num_layers=1\n","    # dropout3: dropout on hidden state of RNN to linear layer\n","    RNNseq_model = RNNSequenceModel(num_classes=2, embedding_dim=300+1024, hidden_size=300,\n","                                    num_layers=1, bidir=True,\n","                                    dropout1=0.5, dropout2=0, dropout3=0.2)\n","    # Move the model to the GPU if available\n","    if using_GPU:\n","        RNNseq_model = RNNseq_model.cuda()\n","    # Set up criterion for calculating loss\n","    loss_criterion = nn.NLLLoss()\n","    # Set up an optimizer for updating the parameters of the rnn_clf\n","    rnn_optimizer = optim.Adam(RNNseq_model.parameters(), lr=0.001)\n","    # Number of epochs (passes through the dataset) to train the model for.\n","    num_epochs = 10\n","\n","    '''\n","    3. 2\n","    train model\n","    '''\n","    train_loss = []\n","    val_loss = []\n","    performance_matrix = None\n","    val_f1 = []\n","    val_p = []\n","    val_r = []\n","    val_acc = []\n","    train_f1 = []\n","    # A counter for the number of gradient updates\n","    num_iter = 0\n","    model_index = 0\n","    comparable = []\n","    for epoch in range(num_epochs):\n","        print(\"Starting epoch {}\".format(epoch + 1))\n","        for (__, example_text, example_lengths, labels) in train_dataloader_trofi:\n","            example_text = Variable(example_text)\n","            example_lengths = Variable(example_lengths)\n","            labels = Variable(labels)\n","            if using_GPU:\n","                example_text = example_text.cuda()\n","                example_lengths = example_lengths.cuda()\n","                labels = labels.cuda()\n","            # predicted shape: (batch_size, seq_len, 2)\n","            predicted = RNNseq_model(example_text, example_lengths)\n","            batch_loss = loss_criterion(predicted.view(-1, 2), labels.view(-1))\n","            rnn_optimizer.zero_grad()\n","            batch_loss.backward()\n","            rnn_optimizer.step()\n","            num_iter += 1\n","            # Calculate validation and training set loss and accuracy every 200 gradient updates\n","            if num_iter % 200 == 0:\n","                avg_eval_loss, performance_matrix = evaluate(idx2pos, val_dataloader_trofi, RNNseq_model,\n","                                                             loss_criterion, using_GPU)\n","                val_loss.append(avg_eval_loss)\n","                val_p.append(performance_matrix[1][0])\n","                val_r.append(performance_matrix[1][1])\n","                val_f1.append(performance_matrix[1][2])\n","                val_acc.append(performance_matrix[1][3])\n","                print(\"Iteration {}. Validation Loss {}.\".format(num_iter, avg_eval_loss))\n","#                 avg_eval_loss, performance_matrix = evaluate(idx2pos, train_dataloader_trofi, RNNseq_model,\n","#                                                              loss_criterion, using_GPU)\n","#                 train_loss.append(avg_eval_loss)\n","#                 train_f1.append(performance_matrix[1][1])\n","#                 print(\"Iteration {}. Training Loss {}.\".format(num_iter, avg_eval_loss))\n","    print(\"Training done for fold {}\".format(i))\n","\n","    \"\"\"\n","    3.3\n","    plot the training process: MET F1 and losses for validation and training dataset\n","    \"\"\"\n","#     plt.figure(0)\n","#     plt.title('F1 for TroFI dataset on fold ' + str(i))\n","#     plt.xlabel('iteration (unit:200)')\n","#     plt.ylabel('F1')\n","#     plt.plot(val_f1, 'g')\n","#     #     plt.plot(train_f1, 'b')\n","#     plt.legend(['Validation F1', 'Training F1'], loc='upper right')\n","#     plt.show()\n","\n","#     plt.figure(1)\n","#     plt.title('Loss for TroFi dataset on fold ' + str(i))\n","#     plt.xlabel('iteration (unit:200)')\n","#     plt.ylabel('Loss')\n","#     plt.plot(val_loss, 'g')\n","#     #     plt.plot(train_loss, 'b')\n","#     plt.legend(['Validation loss', 'Training loss'], loc='upper right')\n","#     plt.show()\n","\n","    \"\"\"\n","    store the best f1\n","    \"\"\"\n","    print('val_f1: ', val_f1)\n","    idx = 0\n","    if math.isnan(max(val_f1)):\n","        optimal_f1s.append(max(val_f1[6:]))\n","        idx = val_f1.index(optimal_f1s[-1])\n","        optimal_ps.append(val_p[idx])\n","        optimal_rs.append(val_r[idx])\n","        optimal_accs.append(val_acc[idx])\n","    else:\n","        optimal_f1s.append(max(val_f1))\n","        idx = val_f1.index(optimal_f1s[-1])\n","        optimal_ps.append(val_p[idx])\n","        optimal_rs.append(val_r[idx])\n","        optimal_accs.append(val_acc[idx])"],"metadata":{"id":"XXxTTXbwpcNy"},"id":"XXxTTXbwpcNy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","print out the performance\n","plot the performance on each fold\n","\"\"\"\n","print('F1 on TroFi by 10-fold = ', optimal_f1s)\n","print('Precision on TroFi = ', np.mean(np.array(optimal_ps)))\n","print('Recall on TroFi = ', np.mean(np.array(optimal_rs)))\n","print('F1 on TroFi = ', np.mean(np.array(optimal_f1s)))\n","print('Accuracy on TroFi = ', np.mean(np.array(optimal_accs)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TU5vw3n91R5_","executionInfo":{"status":"ok","timestamp":1646362740759,"user_tz":360,"elapsed":109,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"3aa880de-f0b9-41a6-c65a-0b8bb648ceec"},"id":"TU5vw3n91R5_","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 on TroFi by 10-fold =  [74.82993197278911, 71.25, 68.96551724137932, 68.24925816023739, 68.85245901639344, 70.93023255813954, 70.15384615384616, 72.38095238095237, 71.9242902208202, 71.21661721068249]\n","Precision on TroFi =  70.44295206262248\n","Recall on TroFi =  71.46397164548841\n","F1 on TroFi =  70.875310491524\n","Accuracy on TroFi =  74.36997319034852\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","gao_scores = [70.7, 71.6, 71.1, 74.6]\n","our_scores = [np.mean(np.array(optimal_ps)),\n","  np.mean(np.array(optimal_rs)),\n","  np.mean(np.array(optimal_f1s)),\n","  np.mean(np.array(optimal_accs))]\n","our_scores = [round(score,1) for score in our_scores]\n","all_scores = [gao_scores, our_scores]\n","all_scores_df = pd.DataFrame(all_scores, columns= ['P', 'R', 'F1', 'Acc'], index=['Gao et al', 'US'])\n","print(\"trofi seq model: classification task\\n\")\n","all_scores_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":146},"id":"8Lb72trY3x_d","executionInfo":{"status":"ok","timestamp":1646363259520,"user_tz":360,"elapsed":109,"user":{"displayName":"Christie Ibaraki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIFzH4hTxQQ5yaIkcpfIR3eeP_amQmg2nF-axQHQ=s64","userId":"17023704871620396360"}},"outputId":"8ac53b44-8bbb-4f1b-fa4e-92560eca9b7b"},"id":"8Lb72trY3x_d","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["trofi seq model: classification task\n","\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-dda5a557-9da6-4d3e-84c0-e02abbf3c9df\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>P</th>\n","      <th>R</th>\n","      <th>F1</th>\n","      <th>Acc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Gao et al</th>\n","      <td>70.7</td>\n","      <td>71.6</td>\n","      <td>71.1</td>\n","      <td>74.6</td>\n","    </tr>\n","    <tr>\n","      <th>US</th>\n","      <td>70.4</td>\n","      <td>71.5</td>\n","      <td>70.9</td>\n","      <td>74.4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dda5a557-9da6-4d3e-84c0-e02abbf3c9df')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-dda5a557-9da6-4d3e-84c0-e02abbf3c9df button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-dda5a557-9da6-4d3e-84c0-e02abbf3c9df');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["              P     R    F1   Acc\n","Gao et al  70.7  71.6  71.1  74.6\n","US         70.4  71.5  70.9  74.4"]},"metadata":{},"execution_count":24}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"colab":{"name":"replicate_trofi_seq_model_for_classification.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}