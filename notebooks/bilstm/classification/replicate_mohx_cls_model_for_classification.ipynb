{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"replicate_mohx_cls_model_for_classification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZL01yWg6PWDG","outputId":"b6c62b66-3826-471b-ea56-a5688ddc5483"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# mount google drive \n","from google.colab import drive\n","ROOT = '/content/drive'\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import sys\n","from os.path import join \n","repo_dir = '/content/drive/MyDrive/metaphor-detection'"],"metadata":{"id":"SM0CV7h9Pn8C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## directories for resources\n","data_dir = repo_dir + '/resources/metaphor-in-context/data/'\n","glove_dir = repo_dir + '/resources/glove/'\n","elmo_dir = repo_dir + '/resources/elmo/'\n"],"metadata":{"id":"JEcWut_bPv-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# installing the requirements\n","%cd 'drive/MyDrive/metaphor-detection/' \n","#!pip install allennlp\n","#!pip install -r gao-g-requirements.txt\n","#!pip install --upgrade google-cloud-storage"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kI3D0ZE-P80O","outputId":"5446eebb-cf36-4afe-e8ef-0c6ec7c07bbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive/MyDrive/metaphor-detection/'\n","/content/drive/MyDrive/metaphor-detection\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","\n","from core.gao_files.classification.model import RNNSequenceClassifier\n","import time\n","import matplotlib\n","from core.gao_files.classification.util import *\n","from core.data.gao_data import *\n","import h5py\n","import math\n","import numpy as np"],"metadata":{"id":"IKl_22p9QTyc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Preperation"],"metadata":{"id":"sEH2Y0yIVUSq"}},{"cell_type":"code","source":["### Read MOH-x Data \n","data_dir = os.path.join(\"resources\", \"metaphor-in-context\", \"data\")\n","data_container = ExperimentData(data_dir)\n","data_container.read_moh_x_data(to_pandas = False)\n","moh_x_data = data_container.moh_x_formatted_svo_cleaned\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vNgZNtHrS4N9","outputId":"d53d2e77-f4b4-4dad-8284-bacb990d02c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MOH-X formatted svo nrow: 647\n","MOH-X formatted svo cleaned nrow: 647\n"]}]},{"cell_type":"code","source":["### Pre-Process Data\n","vocab = get_vocab(moh_x_data)\n","word2idx, idx2word = get_word2idx_idx2word(vocab)\n","glove_embeddings = get_embedding_matrix(glove_dir + 'glove840B300d.txt', \n","                                        word2idx, \n","                                        idx2word, \n","                                        normalization=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQLbTn4iVFvI","outputId":"eed49ce3-793a-4805-a86c-a0508031f01d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab size:  453\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2196017/2196017 [00:47<00:00, 46028.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Number of pre-trained word vectors loaded:  453\n","Embeddings mean:  -0.0009290720336139202\n","Embeddings stdev:  0.38682886958122253\n"]}]},{"cell_type":"code","source":["NUM_SUFFIX_TAG = 2\n","elmo_embeddings = h5py.File(elmo_dir + 'MOH-X_cleaned.hdf5', 'r')\n","suffix_embeddings = nn.Embedding(NUM_SUFFIX_TAG, 50)"],"metadata":{"id":"ENtoXss4VqYA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Embedding Datasets\n","embedded_data = [[embed_sequence(data[3].strip(), int(data[4]), word2idx, glove_embeddings, elmo_embeddings, suffix_embeddings), int(data[-1])] for data in moh_x_data]\n","sentences = [data[0] for data in embedded_data]\n","labels = [data[1] for data in embedded_data]"],"metadata":{"id":"eP9Yq78ZaR5k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## K-Fold Training"],"metadata":{"id":"bWFqrO4IeJ_S"}},{"cell_type":"code","source":["print(f\"Data Length is {len(moh_x_data)}\")\n","NUMBER_FOLD = 10\n","fold_size = round(len(moh_x_data) / NUMBER_FOLD)\n","print(f\"Each fold size is {fold_size}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y9r2c1theMCs","outputId":"7cd509d0-c820-40ff-dbc6-8c4e2d11289c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Length is 647\n","Each fold size is 65\n"]}]},{"cell_type":"code","source":["folds = []\n","for i in range(NUMBER_FOLD):\n","    folds.append((sentences[i * fold_size:(i + 1) * fold_size], labels[i * fold_size: (i + 1) * fold_size]))\n"],"metadata":{"id":"m2p3ypuYelV9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimal_f1s = []\n","accuracies = []\n","precisions = []\n","recalls = []\n","BATCH_SIZE = 10\n","using_GPU = True\n","NUM_EPOCHS = 10\n","for i in range(NUMBER_FOLD):\n","    ### DATA BATCHING\n","    training_sentences = []\n","    training_labels = []\n","    for j in range(NUMBER_FOLD):\n","        if j != i:\n","            training_sentences.extend(folds[j][0])\n","            training_labels.extend(folds[j][1])\n","    training_dataset_mohX = TextDatasetWithGloveElmoSuffix(training_sentences, \n","                                                           training_labels)\n","    val_dataset_mohX = TextDatasetWithGloveElmoSuffix(folds[i][0], \n","                                                      folds[i][1])\n","\n","    # Data-related hyperparameters\n","    # Set up a DataLoader for the training, validation, and test dataset\n","    train_dataloader_mohX = DataLoader(dataset=training_dataset_mohX, \n","                                       batch_size=BATCH_SIZE, \n","                                       shuffle=True,\n","                                      collate_fn=TextDatasetWithGloveElmoSuffix\n","                                                .collate_fn)\n","    val_dataloader_mohX = DataLoader(dataset=val_dataset_mohX, \n","                                     batch_size=BATCH_SIZE, \n","                                     shuffle=True,\n","                                      collate_fn=TextDatasetWithGloveElmoSuffix\n","                                                .collate_fn)\n","    rnn_clf = RNNSequenceClassifier(num_classes=2, \n","                                    embedding_dim=300+1024+50, \n","                                    hidden_size=300, num_layers=1, \n","                                    bidir=True,\n","                                    dropout1=0.2, dropout2=0, dropout3=0.2)\n","    nll_criterion = nn.NLLLoss()\n","    if using_GPU:\n","        rnn_clf = rnn_clf.cuda()\n","        nll_criterion = nll_criterion.cuda()\n","\n","    rnn_clf_optimizer = optim.SGD(rnn_clf.parameters(), lr=0.02, momentum=0.9)\n","    #### TRAIN ####\n","    training_loss = []\n","    val_loss = []\n","    val_p = []\n","    val_r = []\n","    val_acc = []\n","    training_f1 = []\n","    val_f1 = []\n","    num_iter = 0\n","    for epoch in range(NUM_EPOCHS):\n","        print(\"-----Starting epoch {}------\".format(epoch + 1))\n","        now = time.time()\n","        for (example_text, example_lengths, labels) in train_dataloader_mohX:\n","            example_text = Variable(example_text)\n","            example_lengths = Variable(example_lengths)\n","            labels = Variable(labels)\n","            if using_GPU:\n","                example_text = example_text.cuda()\n","                example_lengths = example_lengths.cuda()\n","                labels = labels.cuda()\n","\n","            # predicted shape: (batch_size, 2)\n","            predicted = rnn_clf(example_text, example_lengths)\n","            batch_loss = nll_criterion(predicted, labels)\n","            rnn_clf_optimizer.zero_grad()\n","            batch_loss.backward()\n","            rnn_clf_optimizer.step()\n","            num_iter += 1\n","            # Calculate validation and training set loss and accuracy every 200 gradient updates\n","            if num_iter % 200 == 0:\n","              avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1 = evaluate(val_dataloader_mohX, rnn_clf,\n","                                                                                    nll_criterion, using_GPU)\n","              val_loss.append(avg_eval_loss)\n","              val_f1.append(f1)\n","              val_p.append(precision)\n","              val_r.append(recall)\n","              val_acc.append(eval_accuracy.item())\n","              print(\n","                  \"\"\"Iteration {}. Validation Loss {}. Validation Accuracy {}. \n","                    Validation Precision {}. Validation Recall {}. \n","                    Validation F1 {}. Validation class-wise F1 {}.\"\"\".format(\n","              num_iter, avg_eval_loss, eval_accuracy, precision, recall, f1, fus_f1))\n","                        \n","            \n","    print(\"Training done for fold {}\".format(i))\n","    idx = 0\n","    try:\n","      if math.isnan(max(val_f1)):\n","          optimal_f1s.append(max(val_f1[6:]))\n","          idx = val_f1.index(optimal_f1s[-1])\n","          precisions.append(val_p[idx])\n","          recalls.append(val_r[idx])\n","          accuracies.append(val_acc[idx])\n","      else:\n","          optimal_f1s.append(max(val_f1))\n","          idx = val_f1.index(optimal_f1s[-1])\n","          precisions.append(val_p[idx])\n","          recalls.append(val_r[idx])\n","          accuracies.append(val_acc[idx])\n","    except:\n","      print(idx)\n","      print(val_p)\n","                \n","\n","print('F1 on MOH-X by 10-fold = ', optimal_f1s)\n","print('F1 on MOH-X = ', np.mean(np.array(optimal_f1s)))\n","print('precisions on MOH-X = ', np.mean(np.array(precisions)))\n","print('recalls on MOH-X = ', np.mean(np.array(recalls)))\n","print('accuracies on MOH-X = ', np.mean(np.array(accuracies)))\n"],"metadata":{"id":"vu1PnX_mfR4e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gao_results = [75.3, 84.3, 79.1, 78.5]\n","our_results = [round(np.mean(np.array(precisions)), 2),\n","               round(np.mean(np.array(recalls)), 2),\n","               round(np.mean(np.array(optimal_f1s)), 2),\n","               round(np.mean(np.array(accuracies)), 2)]"],"metadata":{"id":"lRS1H1-tAyRh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = pd.DataFrame([gao_results, our_results], \n","                       columns = ['P', \"R\", \"F1\", \"Acc\"],\n","                       index = [\"GAO\", \"US\"])"],"metadata":{"id":"lak7BE0dB1tz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"PbngdCnnB2XM","outputId":"93d124a3-bcd3-4864-cb74-8bfef35c4ec2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-9cab79e7-0cd4-4069-b190-bb8f21f0201d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>P</th>\n","      <th>R</th>\n","      <th>F1</th>\n","      <th>Acc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>GAO</th>\n","      <td>75.30</td>\n","      <td>84.30</td>\n","      <td>79.1</td>\n","      <td>78.50</td>\n","    </tr>\n","    <tr>\n","      <th>US</th>\n","      <td>75.34</td>\n","      <td>75.41</td>\n","      <td>74.8</td>\n","      <td>75.15</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9cab79e7-0cd4-4069-b190-bb8f21f0201d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9cab79e7-0cd4-4069-b190-bb8f21f0201d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9cab79e7-0cd4-4069-b190-bb8f21f0201d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         P      R    F1    Acc\n","GAO  75.30  84.30  79.1  78.50\n","US   75.34  75.41  74.8  75.15"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":[""],"metadata":{"id":"bVMvNJkkCIg8"},"execution_count":null,"outputs":[]}]}