{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "replicate_mohx_sequence.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_bVSLfpL12W",
        "outputId": "8b0fae2a-fe8f-4ac3-936e-b8b02024a85c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "ROOT = '/content/drive'\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from os.path import join \n",
        "repo_dir = '/content/drive/MyDrive/metaphor-detection'"
      ],
      "metadata": {
        "id": "UVpWOogpMTE9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## directories for resources\n",
        "data_dir = repo_dir + '/resources/metaphor-in-context/data/'\n",
        "glove_dir = repo_dir + '/resources/glove/'\n",
        "elmo_dir = repo_dir + '/resources/elmo/'"
      ],
      "metadata": {
        "id": "3Cco5NznMXLb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## installing the requirements\n",
        "%cd 'drive/MyDrive/metaphor-detection/' \n",
        "#!pip install allennlp\n",
        "#!pip install -r gao-g-requirements.txt\n",
        "#!pip install --upgrade google-cloud-storage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXgKE0zYMYUa",
        "outputId": "a5c3893e-616e-4f95-adc9-d6126e9e9d73"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/metaphor-detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from core.gao_files.sequence.model import RNNSequenceModel\n",
        "import time\n",
        "import matplotlib\n",
        "from core.gao_files.sequence.util import *\n",
        "from core.gao_files.sequence.util import TextDatasetWithGloveElmoSuffix as TextDataset\n",
        "from core.data.gao_data import *\n",
        "import h5py\n",
        "import math\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "76Jhoq62MbdE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preperation"
      ],
      "metadata": {
        "id": "Jht9gQOrNPpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Read MOH-x Data \n",
        "data_dir = os.path.join(\"resources\", \"metaphor-in-context\", \"data\")"
      ],
      "metadata": {
        "id": "VLachZPZNIY-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_mohx = []\n",
        "\n",
        "with open(data_dir + '/MOH-X/MOH-X_formatted_svo_cleaned.csv') as f:\n",
        "    # arg1  \targ2\tverb\tsentence\tverb_idx\tlabel\n",
        "    lines = csv.reader(f)\n",
        "    next(lines)\n",
        "    for line in lines:\n",
        "        sentence = line[3]\n",
        "        label_seq = [0] * len(sentence.split())\n",
        "        pos_seq = [0] * len(label_seq)\n",
        "        verb_idx = int(line[4])\n",
        "        verb_label = int(line[5])\n",
        "        label_seq[verb_idx] = verb_label\n",
        "        pos_seq[verb_idx] = 1   # idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n",
        "        raw_mohx.append([sentence.strip(), label_seq, pos_seq])\n"
      ],
      "metadata": {
        "id": "eFQqXuJrNMWy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Sample data format\n",
        "raw_mohx[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsXnc8bhN8BW",
        "outputId": "ac3cf2dd-ddf2-4e3c-c8b3-56f94aa8f330"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He absorbed the knowledge or beliefs of his tribe .',\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab is a set of words\n",
        "vocab = get_vocab(raw_mohx)\n",
        "# two dictionaries. <PAD>: 0, <UNK>: 1\n",
        "word2idx, idx2word = get_word2idx_idx2word(vocab)\n",
        "# glove_embeddings a nn.Embeddings\n",
        "glove_embeddings = get_embedding_matrix(glove_dir + 'glove840B300d.txt' \n",
        "                                        ,word2idx, idx2word, normalization=False)\n",
        "# elmo_embeddings\n",
        "# set elmos_mohx=None to exclude elmo vectors. Also need to change the embedding_dim in later model initialization\n",
        "elmos_mohx = h5py.File(elmo_dir + 'MOH-X_cleaned.hdf5', 'r')\n",
        "\n",
        "random.seed(0)\n",
        "random.shuffle(raw_mohx)\n",
        "\n",
        "# second argument is the post sequence, which we don't need\n",
        "embedded_mohx = [[embed_indexed_sequence(example[0], example[2], word2idx,\n",
        "                                      glove_embeddings, elmos_mohx, None),\n",
        "                       example[2], example[1]]\n",
        "                      for example in raw_mohx]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkqQ0wwKN91Q",
        "outputId": "9538a7fc-a03c-4ec6-9c49-ccb6c03f7be3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size:  1732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2196017/2196017 [00:48<00:00, 45172.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pre-trained word vectors loaded:  1730\n",
            "Embeddings mean:  -0.0001946780103025958\n",
            "Embeddings stdev:  0.3732253909111023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [data[0] for data in embedded_mohx]\n",
        "poss = [data[1] for data in embedded_mohx]\n",
        "labels = [data[2] for data in embedded_mohx]"
      ],
      "metadata": {
        "id": "-5dwd9ovPZ8u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Fold Training"
      ],
      "metadata": {
        "id": "g5wGKGTkPu4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using_GPU = True\n",
        "ten_folds = []\n",
        "fold_size = int(647 / 10)\n",
        "for i in range(10):\n",
        "    ten_folds.append((sentences[i * fold_size:(i + 1) * fold_size],\n",
        "                      poss[i * fold_size:(i + 1) * fold_size],\n",
        "                      labels[i * fold_size:(i + 1) * fold_size]))\n",
        "\n",
        "idx2pos = {0: 'words that are not focus verbs', 1: 'focus verb'}\n",
        "\n",
        "optimal_f1s = []\n",
        "optimal_ps = []\n",
        "optimal_rs = []\n",
        "optimal_accs = []\n",
        "predictions_all = []\n",
        "for i in range(10):\n",
        "    '''\n",
        "    2. 3\n",
        "    set up Dataloader for batching\n",
        "    '''\n",
        "    training_sentences = []\n",
        "    training_labels = []\n",
        "    training_poss = []\n",
        "    for j in range(10):\n",
        "        if j != i:\n",
        "            training_sentences.extend(ten_folds[j][0])\n",
        "            training_poss.extend(ten_folds[j][1])\n",
        "            training_labels.extend(ten_folds[j][2])\n",
        "    training_dataset_mohx = TextDataset(training_sentences, training_poss, training_labels)\n",
        "    val_dataset_mohx = TextDataset(ten_folds[i][0], ten_folds[i][1], ten_folds[i][2])\n",
        "\n",
        "    # Data-related hyperparameters\n",
        "    batch_size = 10\n",
        "    # Set up a DataLoader for the training, validation, and test dataset\n",
        "    train_dataloader_mohx = DataLoader(dataset=training_dataset_mohx, batch_size=batch_size, shuffle=True,\n",
        "                                       collate_fn=TextDataset.collate_fn)\n",
        "    val_dataloader_mohx = DataLoader(dataset=val_dataset_mohx, batch_size=batch_size, shuffle=False,\n",
        "                                     collate_fn=TextDataset.collate_fn)\n",
        "\n",
        "    \"\"\"\n",
        "    3. Model training\n",
        "    \"\"\"\n",
        "    '''\n",
        "    3. 1 \n",
        "    set up model, loss criterion, optimizer\n",
        "    '''\n",
        "    # Instantiate the model\n",
        "    # embedding_dim = glove + elmo + suffix indicator\n",
        "    # dropout1: dropout on input to RNN\n",
        "    # dropout2: dropout in RNN; would be used if num_layers=1\n",
        "    # dropout3: dropout on hidden state of RNN to linear layer\n",
        "    RNNseq_model = RNNSequenceModel(num_classes=2, embedding_dim=300+1024, hidden_size=300,\n",
        "                                    num_layers=1, bidir=True,\n",
        "                                    dropout1=0.5, dropout2=0, dropout3=0)\n",
        "    # Move the model to the GPU if available\n",
        "    if using_GPU:\n",
        "        RNNseq_model = RNNseq_model.cuda()\n",
        "    # Set up criterion for calculating loss\n",
        "    loss_criterion = nn.NLLLoss()\n",
        "    # Set up an optimizer for updating the parameters of the rnn_clf\n",
        "    rnn_optimizer = optim.Adam(RNNseq_model.parameters(), lr=0.001)\n",
        "    # Number of epochs (passes through the dataset) to train the model for.\n",
        "    num_epochs = 10\n",
        "\n",
        "    '''\n",
        "    3. 2\n",
        "    train model\n",
        "    '''\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    performance_matrix = None\n",
        "    val_f1 = []\n",
        "    val_p = []\n",
        "    val_r = []\n",
        "    val_acc = []\n",
        "    train_f1 = []\n",
        "    # A counter for the number of gradient updates\n",
        "    num_iter = 0\n",
        "    model_index = 0\n",
        "    comparable = []\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Starting epoch {}\".format(epoch + 1))\n",
        "        for (__, example_text, example_lengths, labels) in train_dataloader_mohx:\n",
        "            example_text = Variable(example_text)\n",
        "            example_lengths = Variable(example_lengths)\n",
        "            labels = Variable(labels)\n",
        "            if using_GPU:\n",
        "                example_text = example_text.cuda()\n",
        "                example_lengths = example_lengths.cuda()\n",
        "                labels = labels.cuda()\n",
        "            # predicted shape: (batch_size, seq_len, 2)\n",
        "            predicted = RNNseq_model(example_text, example_lengths)\n",
        "            batch_loss = loss_criterion(predicted.view(-1, 2), labels.view(-1))\n",
        "            rnn_optimizer.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            rnn_optimizer.step()\n",
        "            num_iter += 1\n",
        "            # Calculate validation and training set loss and accuracy every 200 gradient updates\n",
        "            if num_iter % 50 == 0:\n",
        "                avg_eval_loss, performance_matrix = evaluate(idx2pos, val_dataloader_mohx, RNNseq_model,\n",
        "                                                             loss_criterion, using_GPU)\n",
        "                val_loss.append(avg_eval_loss)\n",
        "                val_p.append(performance_matrix[1][0])\n",
        "                val_r.append(performance_matrix[1][1])\n",
        "                val_f1.append(performance_matrix[1][2])\n",
        "                val_acc.append(performance_matrix[1][3])\n",
        "                #print(\"Iteration {}. Validation Loss {}.\".format(num_iter, avg_eval_loss))\n",
        "    #print(\"Training done for fold {}\".format(i))\n",
        "    \"\"\"\n",
        "    store the best f1\n",
        "    \"\"\"\n",
        "    print('val_f1: ', val_f1)\n",
        "    idx = 0\n",
        "    if math.isnan(max(val_f1)):\n",
        "        optimal_f1s.append(max(val_f1[6:]))\n",
        "        idx = val_f1.index(optimal_f1s[-1])\n",
        "        optimal_ps.append(val_p[idx])\n",
        "        optimal_rs.append(val_r[idx])\n",
        "        optimal_accs.append(val_acc[idx])\n",
        "    else:\n",
        "        optimal_f1s.append(max(val_f1))\n",
        "        idx = val_f1.index(optimal_f1s[-1])\n",
        "        optimal_ps.append(val_p[idx])\n",
        "        optimal_rs.append(val_r[idx])\n",
        "        optimal_accs.append(val_acc[idx])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "print out the performance\n",
        "plot the performance on each fold\n",
        "\"\"\"\n",
        "print('F1 on MOH-X by 10-fold = ', optimal_f1s)\n",
        "print('Precision on MOH-X = ', np.mean(np.array(optimal_ps)))\n",
        "print('Recall on MOH-X = ', np.mean(np.array(optimal_rs)))\n",
        "print('F1 on MOH-X = ', np.mean(np.array(optimal_f1s)))\n",
        "print('Accuracy on MOH-X = ', np.mean(np.array(optimal_accs)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyuW9pMYP8uh",
        "outputId": "1b9290d6-4622-4352-f4ed-6cf7f39eac35"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/metaphor-detection/core/gao_files/sequence/util.py:218: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  eval_text = Variable(eval_text, volatile=True)\n",
            "/content/drive/MyDrive/metaphor-detection/core/gao_files/sequence/util.py:219: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  eval_lengths = Variable(eval_lengths, volatile=True)\n",
            "/content/drive/MyDrive/metaphor-detection/core/gao_files/sequence/util.py:220: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  eval_labels = Variable(eval_labels, volatile=True)\n",
            "/content/drive/MyDrive/metaphor-detection/core/gao_files/sequence/util.py:351: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  precision = 100 * grid[1, 1] / np.sum(grid[1])\n",
            "/content/drive/MyDrive/metaphor-detection/core/gao_files/sequence/util.py:352: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  recall = 100 * grid[1, 1] / np.sum(grid[:, 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 66.66666666666667 56.0 60.869565217391305 71.875\n",
            "Starting epoch 2\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 68.0 68.0 68.0 75.0\n",
            "Starting epoch 3\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 76.19047619047619 64.0 69.56521739130434 78.125\n",
            "Starting epoch 4\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 51.282051282051285 80.0 62.50000000000001 62.5\n",
            "Starting epoch 5\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 66.66666666666667 64.0 65.3061224489796 73.4375\n",
            "Starting epoch 6\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 46.51162790697674 80.0 58.8235294117647 56.25\n",
            "Starting epoch 7\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 72.72727272727273 64.0 68.08510638297872 76.5625\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 56.25 72.0 63.1578947368421 67.1875\n",
            "Starting epoch 8\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 60.0 72.0 65.45454545454545 70.3125\n",
            "Starting epoch 9\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 54.54545454545455 72.0 62.06896551724138 65.625\n",
            "Starting epoch 10\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 56.25 72.0 63.1578947368421 67.1875\n",
            "val_f1:  [60.869565217391305, 68.0, 69.56521739130434, 62.50000000000001, 65.3061224489796, 58.8235294117647, 68.08510638297872, 63.1578947368421, 65.45454545454545, 62.06896551724138, 63.1578947368421]\n",
            "Starting epoch 1\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 86.95652173913044 58.8235294117647 70.17543859649123 73.4375\n",
            "Starting epoch 2\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 68.29268292682927 82.3529411764706 74.66666666666667 70.3125\n",
            "Starting epoch 3\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 70.0 61.76470588235294 65.62500000000001 65.625\n",
            "Starting epoch 4\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 71.42857142857143 88.23529411764706 78.94736842105263 75.0\n",
            "Starting epoch 5\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 80.0 70.58823529411765 75.00000000000001 75.0\n",
            "Starting epoch 6\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78260869565217\n",
            "PRFA performance for  focus verb 76.47058823529412 76.47058823529412 76.47058823529412 75.0\n",
            "Starting epoch 7\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 73.6842105263158 41.1764705882353 52.83018867924529 60.9375\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 73.07692307692308 55.88235294117647 63.33333333333333 65.625\n",
            "Starting epoch 8\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 73.07692307692308 55.88235294117647 63.33333333333333 65.625\n",
            "Starting epoch 9\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 74.07407407407408 58.8235294117647 65.57377049180327 67.1875\n",
            "Starting epoch 10\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 79.41176470588235 79.41176470588235 79.41176470588235 78.125\n",
            "val_f1:  [70.17543859649123, 74.66666666666667, 65.62500000000001, 78.94736842105263, 75.00000000000001, 76.47058823529412, 52.83018867924529, 63.33333333333333, 63.33333333333333, 65.57377049180327, 79.41176470588235]\n",
            "Starting epoch 1\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 54.285714285714285 67.85714285714286 60.31746031746032 60.9375\n",
            "Starting epoch 2\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.75369458128078\n",
            "PRFA performance for  focus verb 73.07692307692308 67.85714285714286 70.37037037037038 75.0\n",
            "Starting epoch 3\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.75369458128078\n",
            "PRFA performance for  focus verb 68.96551724137932 71.42857142857143 70.17543859649123 73.4375\n",
            "Starting epoch 4\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 81.81818181818181 64.28571428571429 72.0 78.125\n",
            "Starting epoch 5\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.75369458128078\n",
            "PRFA performance for  focus verb 65.625 75.0 70.0 71.875\n",
            "Starting epoch 6\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 77.77777777777777 75.0 76.36363636363636 79.6875\n",
            "Starting epoch 7\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 73.33333333333333 78.57142857142857 75.86206896551724 78.125\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 68.18181818181819 53.57142857142857 60.0 68.75\n",
            "Starting epoch 8\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.75369458128078\n",
            "PRFA performance for  focus verb 73.33333333333333 78.57142857142857 75.86206896551724 78.125\n",
            "Starting epoch 9\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 78.26086956521739 64.28571428571429 70.58823529411765 76.5625\n",
            "Starting epoch 10\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 73.91304347826087 60.714285714285715 66.66666666666666 73.4375\n",
            "val_f1:  [60.31746031746032, 70.37037037037038, 70.17543859649123, 72.0, 70.0, 76.36363636363636, 75.86206896551724, 60.0, 75.86206896551724, 70.58823529411765, 66.66666666666666]\n",
            "Starting epoch 1\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 69.23076923076923 60.0 64.28571428571428 68.75\n",
            "Starting epoch 2\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 67.6470588235294 76.66666666666667 71.875 71.875\n",
            "Starting epoch 3\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 65.11627906976744 93.33333333333333 76.71232876712328 73.4375\n",
            "Starting epoch 4\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 64.86486486486487 80.0 71.64179104477613 70.3125\n",
            "Starting epoch 5\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 60.526315789473685 76.66666666666667 67.64705882352942 65.625\n",
            "Starting epoch 6\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 60.97560975609756 83.33333333333333 70.4225352112676 67.1875\n",
            "Starting epoch 7\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 63.41463414634146 86.66666666666667 73.2394366197183 70.3125\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 64.28571428571429 60.0 62.06896551724138 65.625\n",
            "Starting epoch 8\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 64.1025641025641 83.33333333333333 72.46376811594203 70.3125\n",
            "Starting epoch 9\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 60.0 90.0 72.0 67.1875\n",
            "Starting epoch 10\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 58.333333333333336 70.0 63.63636363636363 62.5\n",
            "val_f1:  [64.28571428571428, 71.875, 76.71232876712328, 71.64179104477613, 67.64705882352942, 70.4225352112676, 73.2394366197183, 62.06896551724138, 72.46376811594203, 72.0, 63.63636363636363]\n",
            "Starting epoch 1\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.34354485776805\n",
            "PRFA performance for  focus verb 61.53846153846154 72.72727272727273 66.66666666666667 62.5\n",
            "Starting epoch 2\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.34354485776805\n",
            "PRFA performance for  focus verb 64.28571428571429 81.81818181818181 72.0 67.1875\n",
            "Starting epoch 3\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 71.42857142857143 75.75757575757575 73.52941176470587 71.875\n",
            "Starting epoch 4\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 85.0 51.515151515151516 64.15094339622642 70.3125\n",
            "Starting epoch 5\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 68.57142857142857 72.72727272727273 70.58823529411765 68.75\n",
            "Starting epoch 6\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78118161925602\n",
            "PRFA performance for  focus verb 73.33333333333333 66.66666666666667 69.84126984126983 70.3125\n",
            "Starting epoch 7\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 80.76923076923077 63.63636363636363 71.1864406779661 73.4375\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56236323851203\n",
            "PRFA performance for  focus verb 73.33333333333333 66.66666666666667 69.84126984126983 70.3125\n",
            "Starting epoch 8\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78118161925602\n",
            "PRFA performance for  focus verb 79.3103448275862 69.6969696969697 74.19354838709678 75.0\n",
            "Starting epoch 9\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.56236323851203\n",
            "PRFA performance for  focus verb 72.72727272727273 72.72727272727273 72.72727272727273 71.875\n",
            "Starting epoch 10\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 73.52941176470588 75.75757575757575 74.6268656716418 73.4375\n",
            "val_f1:  [66.66666666666667, 72.0, 73.52941176470587, 64.15094339622642, 70.58823529411765, 69.84126984126983, 71.1864406779661, 69.84126984126983, 74.19354838709678, 72.72727272727273, 74.6268656716418]\n",
            "Starting epoch 1\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.79633401221996\n",
            "PRFA performance for  focus verb 64.28571428571429 50.0 56.25000000000001 56.25\n",
            "Starting epoch 2\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.79633401221996\n",
            "PRFA performance for  focus verb 74.19354838709677 63.888888888888886 68.65671641791045 67.1875\n",
            "Starting epoch 3\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.79633401221996\n",
            "PRFA performance for  focus verb 80.95238095238095 47.22222222222222 59.64912280701755 64.0625\n",
            "Starting epoch 4\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.79633401221996\n",
            "PRFA performance for  focus verb 80.0 55.55555555555556 65.57377049180329 67.1875\n",
            "Starting epoch 5\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.79633401221996\n",
            "PRFA performance for  focus verb 76.0 52.77777777777778 62.29508196721312 64.0625\n",
            "Starting epoch 6\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.79633401221996\n",
            "PRFA performance for  focus verb 82.6086956521739 52.77777777777778 64.40677966101696 67.1875\n",
            "Starting epoch 7\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.79633401221996\n",
            "PRFA performance for  focus verb 77.77777777777777 58.333333333333336 66.66666666666666 67.1875\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.79633401221996\n",
            "PRFA performance for  focus verb 76.92307692307692 55.55555555555556 64.51612903225806 65.625\n",
            "Starting epoch 8\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.79633401221996\n",
            "PRFA performance for  focus verb 75.0 58.333333333333336 65.625 65.625\n",
            "Starting epoch 9\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 82.6086956521739 52.77777777777778 64.40677966101696 67.1875\n",
            "Starting epoch 10\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 79.16666666666667 52.77777777777778 63.33333333333333 65.625\n",
            "val_f1:  [56.25000000000001, 68.65671641791045, 59.64912280701755, 65.57377049180329, 62.29508196721312, 64.40677966101696, 66.66666666666666, 64.51612903225806, 65.625, 64.40677966101696, 63.33333333333333]\n",
            "Starting epoch 1\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 56.52173913043478 76.47058823529412 65.0 56.25\n",
            "Starting epoch 2\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78118161925602\n",
            "PRFA performance for  focus verb 77.5 91.17647058823529 83.78378378378378 81.25\n",
            "Starting epoch 3\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 91.30434782608695 61.76470588235294 73.68421052631578 76.5625\n",
            "Starting epoch 4\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 88.46153846153847 67.6470588235294 76.66666666666666 78.125\n",
            "Starting epoch 5\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 78.37837837837837 85.29411764705883 81.69014084507042 79.6875\n",
            "Starting epoch 6\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 88.88888888888889 70.58823529411765 78.68852459016394 79.6875\n",
            "Starting epoch 7\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 78.78787878787878 76.47058823529412 77.61194029850745 76.5625\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 79.41176470588235 79.41176470588235 79.41176470588235 78.125\n",
            "Starting epoch 8\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 85.29411764705883 85.29411764705883 85.29411764705883 84.375\n",
            "Starting epoch 9\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 77.77777777777777 82.3529411764706 80.0 78.125\n",
            "Starting epoch 10\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 76.3157894736842 85.29411764705883 80.55555555555556 78.125\n",
            "val_f1:  [65.0, 83.78378378378378, 73.68421052631578, 76.66666666666666, 81.69014084507042, 78.68852459016394, 77.61194029850745, 79.41176470588235, 85.29411764705883, 80.0, 80.55555555555556]\n",
            "Starting epoch 1\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 85.0 47.22222222222222 60.71428571428571 65.625\n",
            "Starting epoch 2\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.32735426008969\n",
            "PRFA performance for  focus verb 69.76744186046511 83.33333333333333 75.9493670886076 70.3125\n",
            "Starting epoch 3\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.77578475336323\n",
            "PRFA performance for  focus verb 72.97297297297297 75.0 73.97260273972603 70.3125\n",
            "Starting epoch 4\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 79.41176470588235 75.0 77.14285714285714 75.0\n",
            "Starting epoch 5\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 81.48148148148148 61.111111111111114 69.84126984126983 70.3125\n",
            "Starting epoch 6\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 77.77777777777777 58.333333333333336 66.66666666666666 67.1875\n",
            "Starting epoch 7\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.55156950672645\n",
            "PRFA performance for  focus verb 78.125 69.44444444444444 73.52941176470587 71.875\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.55156950672645\n",
            "PRFA performance for  focus verb 81.81818181818181 75.0 78.26086956521739 76.5625\n",
            "Starting epoch 8\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.77578475336323\n",
            "PRFA performance for  focus verb 81.25 72.22222222222223 76.47058823529412 75.0\n",
            "Starting epoch 9\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.32735426008969\n",
            "PRFA performance for  focus verb 75.0 66.66666666666667 70.58823529411764 68.75\n",
            "Starting epoch 10\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.77578475336323\n",
            "PRFA performance for  focus verb 73.52941176470588 69.44444444444444 71.42857142857143 68.75\n",
            "val_f1:  [60.71428571428571, 75.9493670886076, 73.97260273972603, 77.14285714285714, 69.84126984126983, 66.66666666666666, 73.52941176470587, 78.26086956521739, 76.47058823529412, 70.58823529411764, 71.42857142857143]\n",
            "Starting epoch 1\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 54.54545454545455 64.28571428571429 59.016393442622956 60.9375\n",
            "Starting epoch 2\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 62.857142857142854 78.57142857142857 69.84126984126983 70.3125\n",
            "Starting epoch 3\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 73.91304347826087 60.714285714285715 66.66666666666666 73.4375\n",
            "Starting epoch 4\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 63.63636363636363 75.0 68.85245901639344 70.3125\n",
            "Starting epoch 5\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 65.625 75.0 70.0 71.875\n",
            "Starting epoch 6\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 56.41025641025641 78.57142857142857 65.67164179104478 64.0625\n",
            "Starting epoch 7\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 62.5 71.42857142857143 66.66666666666667 68.75\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 51.111111111111114 82.14285714285714 63.01369863013699 57.8125\n",
            "Starting epoch 8\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 63.63636363636363 75.0 68.85245901639344 70.3125\n",
            "Starting epoch 9\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 73.07692307692308 67.85714285714286 70.37037037037038 75.0\n",
            "Starting epoch 10\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 70.0 75.0 72.41379310344827 75.0\n",
            "val_f1:  [59.016393442622956, 69.84126984126983, 66.66666666666666, 68.85245901639344, 70.0, 65.67164179104478, 66.66666666666667, 63.01369863013699, 68.85245901639344, 70.37037037037038, 72.41379310344827]\n",
            "Starting epoch 1\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78070175438596\n",
            "PRFA performance for  focus verb 56.75675675675676 75.0 64.61538461538461 64.0625\n",
            "Starting epoch 2\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 77.27272727272727 60.714285714285715 68.0 75.0\n",
            "Starting epoch 3\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 77.27272727272727 60.714285714285715 68.0 75.0\n",
            "Starting epoch 4\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78070175438596\n",
            "PRFA performance for  focus verb 64.86486486486487 85.71428571428571 73.84615384615385 73.4375\n",
            "Starting epoch 5\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78070175438596\n",
            "PRFA performance for  focus verb 58.97435897435897 82.14285714285714 68.65671641791045 67.1875\n",
            "Starting epoch 6\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78070175438596\n",
            "PRFA performance for  focus verb 70.0 75.0 72.41379310344827 75.0\n",
            "Starting epoch 7\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78070175438596\n",
            "PRFA performance for  focus verb 67.74193548387096 75.0 71.18644067796609 73.4375\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78070175438596\n",
            "PRFA performance for  focus verb 67.74193548387096 75.0 71.18644067796609 73.4375\n",
            "Starting epoch 8\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs nan nan nan 100.0\n",
            "PRFA performance for  focus verb 62.857142857142854 78.57142857142857 69.84126984126983 70.3125\n",
            "Starting epoch 9\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78070175438596\n",
            "PRFA performance for  focus verb 68.75 78.57142857142857 73.33333333333333 75.0\n",
            "Starting epoch 10\n",
            "------------------------------\n",
            "total_eval_loss.shape torch.Size([])\n",
            "PRFA performance for  words that are not focus verbs 0.0 nan nan 99.78070175438596\n",
            "PRFA performance for  focus verb 71.42857142857143 71.42857142857143 71.42857142857143 75.0\n",
            "val_f1:  [64.61538461538461, 68.0, 68.0, 73.84615384615385, 68.65671641791045, 72.41379310344827, 71.18644067796609, 71.18644067796609, 69.84126984126983, 73.33333333333333, 71.42857142857143]\n",
            "F1 on MOH-X by 10-fold =  [69.56521739130434, 79.41176470588235, 76.36363636363636, 76.71232876712328, 74.6268656716418, 68.65671641791045, 85.29411764705883, 78.26086956521739, 72.41379310344827, 73.84615384615385]\n",
            "Precision on MOH-X =  74.8196422225812\n",
            "Recall on MOH-X =  77.23999660470248\n",
            "F1 on MOH-X =  75.51514634793769\n",
            "Accuracy on MOH-X =  75.9375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "gao_scores = [79.1, 73.5, 75.6, 77.2]\n",
        "our_scores = [np.mean(np.array(optimal_ps)),\n",
        "  np.mean(np.array(optimal_rs)),\n",
        "  np.mean(np.array(optimal_f1s)),\n",
        "  np.mean(np.array(optimal_accs))]\n",
        "our_scores = [round(score,1) for score in our_scores]\n",
        "all_scores = [gao_scores, our_scores]\n",
        "all_scores_df = pd.DataFrame(all_scores, columns= ['P', 'R', 'F1', 'Acc'], index=['Gao et al', 'US'])\n",
        "print(\"Moh-X seq model: classification task\\n\")\n",
        "all_scores_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "FpkmTfI_VlHY",
        "outputId": "a69a170e-9844-4dd6-8fe1-afa4e30be040"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moh-X seq model: classification task\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-14284dd4-12cb-4741-b6d4-fb28e7149467\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>P</th>\n",
              "      <th>R</th>\n",
              "      <th>F1</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Gao et al</th>\n",
              "      <td>79.1</td>\n",
              "      <td>73.5</td>\n",
              "      <td>75.6</td>\n",
              "      <td>77.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>US</th>\n",
              "      <td>74.8</td>\n",
              "      <td>77.2</td>\n",
              "      <td>75.5</td>\n",
              "      <td>75.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14284dd4-12cb-4741-b6d4-fb28e7149467')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-14284dd4-12cb-4741-b6d4-fb28e7149467 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-14284dd4-12cb-4741-b6d4-fb28e7149467');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              P     R    F1   Acc\n",
              "Gao et al  79.1  73.5  75.6  77.2\n",
              "US         74.8  77.2  75.5  75.9"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}